{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Set random seeds to ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Gaussian noise function\n",
    "def apply_gaussian_noise(image, noise_level=3.0):\n",
    "    \"\"\"Add Gaussian noise to a single image\"\"\"\n",
    "    image = image.clone()  # Copy the image to avoid modifying the original data\n",
    "    noise = torch.randn(image.shape) * noise_level  # Generate Gaussian noise\n",
    "    noisy_image = torch.clamp(image + noise, 0, 1)  # Limit the range of pixel values\n",
    "    return noisy_image\n",
    "\n",
    "# data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Standardized\n",
    "])\n",
    "\n",
    "# Load the full MNIST dataset\n",
    "full_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get all the labels\n",
    "labels = [y for _, y in full_trainset]\n",
    "\n",
    "# Hierarchical sampling\n",
    "_, subset_indices = train_test_split(\n",
    "    range(len(full_trainset)),\n",
    "    test_size=1/2,\n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "trainset = Subset(full_trainset, subset_indices)\n",
    "\n",
    "# Check the category distribution\n",
    "from collections import Counter\n",
    "print(\"category distribution:\", Counter([full_trainset[i][1] for i in subset_indices]))\n",
    "\n",
    "# Generate an index of the attacked samples\n",
    "num_train_attack = len(trainset) // 2  # 50% of the training set is attacked to 20% attacked\n",
    "# num_test_attack = len(testset) // 2  # 50% of test sets are attacked\n",
    "\n",
    "attack_train_indices = set(random.sample(range(len(trainset)), num_train_attack))  # Randomly selected samples to add noise\n",
    "train_clean_indices = set(range(len(trainset))) - attack_train_indices  # The other half stays clean\n",
    "\n",
    "#attack_test_indices = set(random.sample(range(len(testset)), num_test_attack))  # Randomly selected samples to add noise\n",
    "#test_clean_indices = set(range(len(testset))) - attack_test_indices  # The other half stays clean\n",
    "\n",
    "# Custom datasets\n",
    "class NoisyMNISTDataset(Dataset):\n",
    "    def __init__(self, dataset, attacked_indices, noise_level=0.1):\n",
    "        self.dataset = dataset\n",
    "        self.attacked_indices = attacked_indices\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        if idx in self.attacked_indices:\n",
    "            img = apply_gaussian_noise(img, self.noise_level)  # 添加高斯噪声 \n",
    "        return img, label\n",
    "\n",
    "# Generate a training set after the attack\n",
    "trainset_noisy = NoisyMNISTDataset(trainset, attack_train_indices, noise_level=5.0)  # Set the noise intensity\n",
    "# testset_noisy = NoisyMNISTDataset(testset, attack_test_indices, noise_level=5.0)\n",
    "\n",
    "# Create a DataLoader\n",
    "trainloader = DataLoader(trainset_noisy, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Print the number of attacked and unattacked samples\n",
    "print(f\"Train Clean Samples: {len(train_clean_indices)}, Attacked Samples: {len(attack_train_indices)}\")\n",
    "# print(f\"Test Clean Samples: {len(test_clean_indices)}, Attacked Samples: {len(attack_test_indices)}\")\n",
    "\n",
    "# Visualize partial samples\n",
    "def show_images(dataset, indices, title):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "    for ax, idx in zip(axes, list(indices)[:5]):  # 5 randomly selected\n",
    "        img, label = dataset[idx]\n",
    "        ax.imshow(img.squeeze(), cmap='gray')\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Shows partially attacked and unattacked samples\n",
    "show_images(trainset_noisy, attack_train_indices, \"Attacked Train Samples\")\n",
    "show_images(trainset_noisy, train_clean_indices, \"Clean Train Samples\")\n",
    "\n",
    "# Generate the training set and test set after the attack (the original code remains unchanged)\n",
    "trainset_noisy = NoisyMNISTDataset(trainset, attack_train_indices, noise_level=5.0)\n",
    "# testset_noisy = NoisyMNISTDataset(testset, attack_test_indices, noise_level=5.0)\n",
    "\n",
    "# Split the training set into attacked and clean subsets\n",
    "train_attack_subset = Subset(trainset_noisy, list(attack_train_indices))  # Attacked training samples\n",
    "train_clean_subset = Subset(trainset_noisy, list(train_clean_indices))    # Clean training samples\n",
    "\n",
    "# test_attack_subset = Subset(testset_noisy, list(attack_test_indices))     # Attacked testing samples\n",
    "# test_clean_subset = Subset(testset_noisy, list(test_clean_indices))       # Clean testing samples\n",
    "\n",
    "# Create the corresponding DataLoader\n",
    "attackTrainloader = DataLoader(train_attack_subset, batch_size=64, shuffle=False)\n",
    "cleanTrainloader = DataLoader(train_clean_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# Print each subset size\n",
    "print(f\"Train - Attack: {len(train_attack_subset)}, Clean: {len(train_clean_subset)}\")\n",
    "# print(f\"Test - Attack: {len(test_attack_subset)}, Clean: {len(test_clean_subset)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Adding USPS dataset\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Custom USPS dataset class\n",
    "class USPSTestSet(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        # Load complete USPS data (9298 samples)\n",
    "        usps = fetch_openml('usps', version=1, parser='auto')\n",
    "        self.data = usps['data'].values.reshape(-1, 16, 16).astype(np.float32)\n",
    "        self.labels = usps['target'].astype(np.int64)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.fromarray(self.data[idx], mode='F')  # Mode 'F' for floating-point image\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, label\n",
    "\n",
    "# Define transforms compatible with MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(28),                  # USPS 16x16 -> MNIST 28x28\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Using MNIST normalization parameters\n",
    "])\n",
    "\n",
    "# Create complete USPS test set\n",
    "usps_testset = USPSTestSet(transform=transform)\n",
    "usps_loader = DataLoader(usps_testset, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "236634af22d2e012",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Define FNN network\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model (with time tracking)\n",
    "epochs = 5\n",
    "total_train_time = 0  # Track total training time\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_start_time = time.time()  # Record epoch start time\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time  # Calculate epoch duration\n",
    "    total_train_time += epoch_time\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "          f\"Loss: {running_loss / len(trainloader):.4f}, \"\n",
    "          f\"Time: {epoch_time:.2f}s\")\n",
    "\n",
    "# Output total training time\n",
    "print(f\"\\nTotal training time: {total_train_time:.2f} seconds\")\n",
    "print(f\"Average time per epoch: {total_train_time/epochs:.2f}s\")\n",
    "time_source  = total_train_time/epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a060af86f61d9771",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Model performance on test set\n",
    "# Validate the model\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "# Ensure input data is on the same device during validation\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a45fca554b610ccb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "## Attribution Calculation - Considering the last layer 128->64->10, computing for the 64-channel layer\n",
    "\n",
    "# Calculate FFN layer attribution\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Automatically select device\n",
    "model.to(device)  # Move model to GPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "109b924ebdfec8cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_ffn_attribution(model, data_loader):\n",
    "    model.eval()\n",
    "    attributions = []\n",
    "    leaky_relu = torch.nn.LeakyReLU(0.1)  # Define activation function\n",
    "    \n",
    "    for images, labels in data_loader:\n",
    "        images = images.to(device)\n",
    "        images.requires_grad_()\n",
    "\n",
    "        # Forward passing\n",
    "        # x = images.view(-1, 28*28)\n",
    "        # x = leaky_relu(model.fc1(x))  # Using LeakyReLU\n",
    "        # fc2_outputs = leaky_relu(model.fc2(x))\n",
    "        # y = model.fc3(fc2_outputs)\n",
    "        # Forward passing (using ReLU)\n",
    "        x = images.view(-1, 28*28)\n",
    "        x = F.relu(model.fc1(x))  # Changed to ReLU\n",
    "        fc2_outputs = F.relu(model.fc2(x))  # Changed to ReLU\n",
    "        y = model.fc3(fc2_outputs)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradient_matrix = []\n",
    "        for i in range(10):  # Compute gradients for each output dimension\n",
    "            model.zero_grad()\n",
    "            grad_i = torch.autograd.grad(\n",
    "                outputs=y[:, i],\n",
    "                inputs=fc2_outputs,\n",
    "                grad_outputs=torch.ones_like(y[:, i]),\n",
    "                retain_graph=True,\n",
    "                create_graph=False\n",
    "            )[0]  # (batch_size, 64)\n",
    "            gradient_matrix.append(grad_i)\n",
    "        \n",
    "        # Stack and aggregate gradients\n",
    "        gradient_matrix = torch.stack(gradient_matrix, dim=1)  # (batch_size, 10, 64)\n",
    "        \n",
    "        # Improved aggregation methods - pure mean and norm are incorrect approaches\n",
    "        \n",
    "        # Method 3: Gradient-weighted by activation values\n",
    "        attribution_weighted = (gradient_matrix.mean(dim=1) * fc2_outputs).abs()\n",
    "        # Max aggregation\n",
    "        attribution_max = (gradient_matrix.max(dim=1)[0] * fc2_outputs).abs()  # (batch, 64)\n",
    "        # Here we select Method 3 as example\n",
    "        \n",
    "        # Top-k clustering\n",
    "        # k = 3\n",
    "        # topk_grads = gradient_matrix.topk(k, dim=1)[0]  # (batch, k, 64)\n",
    "        # attribution = (topk_grads.mean(dim=1) * fc2_outputs).abs()\n",
    "        attributions.append(attribution_max.detach().cpu())\n",
    "    \n",
    "    return torch.cat(attributions, dim=0)  # (num_samples, 64)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9db48500335c482",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Make sure the training data is also on the GPU\n",
    "start_time = time.time()\n",
    "ffn_attributions = compute_ffn_attribution(model, trainloader)\n",
    "end_time = time.time()-start_time\n",
    "print(f\"\\ncal attributions cost time: {end_time:.2f} seconds\")\n",
    "ffn_attributions.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a916afb070412567",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume ffn_attributions is a PyTorch tensor on GPU\n",
    "# 1. Move tensor from GPU to CPU\n",
    "ffn_attributions_cpu = ffn_attributions.cpu()\n",
    "\n",
    "# 2. Convert PyTorch tensor to NumPy array\n",
    "ffn_attributions_np = ffn_attributions_cpu.numpy()\n",
    "\n",
    "# 3. Standardize using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(ffn_attributions_np)\n",
    "\n",
    "# Print standardized data\n",
    "# print(\"Scaled data:\")\n",
    "# print(X_scaled)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6fc4704a54cbd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Perform clustering using K-Means\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)  # Set K=2\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Get cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Cluster analysis\n",
    "import numpy as np\n",
    "\n",
    "# Assume labels is the label array after K-Means clustering\n",
    "# labels is a 1D array with shape (n_samples,)\n",
    "\n",
    "# Get sample indices belonging to cluster 0\n",
    "cluster_0_indices = np.where(labels == 0)[0]\n",
    "print(cluster_0_indices.size)\n",
    "# Get sample indices belonging to cluster 1\n",
    "cluster_1_indices = np.where(labels == 1)[0]\n",
    "print(cluster_1_indices.size)\n",
    "\n",
    "# Print results\n",
    "print(\"Indices of samples in cluster 0:\", cluster_0_indices)\n",
    "print(\"Indices of samples in cluster 1:\", cluster_1_indices)"
   ],
   "id": "4e14c1f87189f857"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# PCA for dimensionality reduction visualization (optional)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# K-Means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Get indices for both clusters\n",
    "cluster_0_indices = np.where(kmeans_labels == 0)[0]\n",
    "cluster_1_indices = np.where(kmeans_labels == 1)[0]\n",
    "\n",
    "print(f\"K-Means clustering results:\")\n",
    "print(f\"Cluster 0 sample count: {len(cluster_0_indices)}\")\n",
    "print(f\"Cluster 1 sample count: {len(cluster_1_indices)}\")\n",
    "\n",
    "# Visualization (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, alpha=0.5)\n",
    "plt.title('K-Means Clustering Results')\n",
    "plt.show()"
   ],
   "id": "576d03ab631735aa"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# GMM clustering\n",
    "gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(X_scaled)\n",
    "\n",
    "# Get indices for both clusters\n",
    "cluster_0_indices = np.where(gmm_labels == 0)[0]\n",
    "cluster_1_indices = np.where(gmm_labels == 1)[0]\n",
    "\n",
    "print(f\"\\nGMM clustering results:\")\n",
    "print(f\"Cluster 0 sample count: {len(cluster_0_indices)}\")\n",
    "print(f\"Cluster 1 sample count: {len(cluster_1_indices)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c8054bdb7eae12b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate the matching between K-Means clustering and manual partitioning\n",
    "attack_set = set(attack_train_indices)  # True attack sample indices\n",
    "clean_set = set(train_clean_indices)    # True clean sample indices\n",
    "\n",
    "cluster_0_set = set(cluster_0_indices)  # Samples in K-Means cluster 0\n",
    "cluster_1_set = set(cluster_1_indices)  # Samples in K-Means cluster 1\n",
    "\n",
    "# Calculate intersections\n",
    "attack_in_cluster_0 = len(attack_set & cluster_0_set)  # Number of true attack samples in Cluster 0\n",
    "attack_in_cluster_1 = len(attack_set & cluster_1_set)  # Number of true attack samples in Cluster 1\n",
    "\n",
    "clean_in_cluster_0 = len(clean_set & cluster_0_set)  # Number of true clean samples in Cluster 0\n",
    "clean_in_cluster_1 = len(clean_set & cluster_1_set)  # Number of true clean samples in Cluster 1\n",
    "\n",
    "# Calculate classification accuracy of K-Means for attack samples\n",
    "attack_accuracy_cluster_0 = (attack_in_cluster_0 / len(attack_set)) * 100\n",
    "attack_accuracy_cluster_1 = (attack_in_cluster_1 / len(attack_set)) * 100\n",
    "\n",
    "clean_accuracy_cluster_0 = (clean_in_cluster_0 / len(clean_set)) * 100\n",
    "clean_accuracy_cluster_1 = (clean_in_cluster_1 / len(clean_set)) * 100\n",
    "\n",
    "# Print comparison results\n",
    "print(\"==== K-Means Clustering vs Manual Labels ====\")\n",
    "print(f\"Cluster 0: {len(cluster_0_indices)} samples\")\n",
    "print(f\"Cluster 1: {len(cluster_1_indices)} samples\\n\")\n",
    "\n",
    "print(f\"Attack Samples in Cluster 0: {attack_in_cluster_0} ({attack_accuracy_cluster_0:.2f}%)\")\n",
    "print(f\"Attack Samples in Cluster 1: {attack_in_cluster_1} ({attack_accuracy_cluster_1:.2f}%)\\n\")\n",
    "\n",
    "print(f\"Clean Samples in Cluster 0: {clean_in_cluster_0} ({clean_accuracy_cluster_0:.2f}%)\")\n",
    "print(f\"Clean Samples in Cluster 1: {clean_in_cluster_1} ({clean_accuracy_cluster_1:.2f}%)\")\n",
    "\n",
    "# Calculate overall classification accuracy of K-Means\n",
    "total_correct = attack_in_cluster_0 + clean_in_cluster_1  # Assuming cluster_0 mainly contains attack samples, cluster_1 mainly clean samples\n",
    "overall_accuracy = (total_correct / len(trainset)) * 100  # Previously train_subset, now changed to trainset\n",
    "\n",
    "print(f\"\\nOverall Clustering Accuracy: {overall_accuracy:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9212d878011f1e6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract samples from cluster 0\n",
    "cluster_0_attributions = ffn_attributions[cluster_0_indices]\n",
    "\n",
    "# Extract samples from cluster 1\n",
    "cluster_1_attributions = ffn_attributions[cluster_1_indices]\n",
    "print(cluster_0_attributions.shape)\n",
    "print(cluster_1_attributions.shape) \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d351b3f4c72b578",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Ensure labels length matches dataset size gam_labels\n",
    "# assert len(labels) == len(trainloader.dataset), \"labels and trainloader.dataset size mismatch!\"\n",
    "\n",
    "# Convert indices to Python list\n",
    "cluster_0_indices = cluster_0_indices.tolist()\n",
    "cluster_1_indices = cluster_1_indices.tolist()\n",
    "\n",
    "# Use Subset to split original dataset by indices\n",
    "dataset_0 = Subset(trainloader.dataset, cluster_0_indices)\n",
    "dataset_1 = Subset(trainloader.dataset, cluster_1_indices)\n",
    "\n",
    "# Create new DataLoaders\n",
    "trainloader_0 = DataLoader(dataset_0, batch_size=64, shuffle=True) #\n",
    "trainloader_1 = DataLoader(dataset_1, batch_size=64, shuffle=True)\n",
    "\n",
    "# Final verification\n",
    "print(f\"Expected Cluster 0 size: {len(cluster_0_indices)}, Actual: {len(trainloader_0.dataset)}\")\n",
    "print(f\"Expected Cluster 1 size: {len(cluster_1_indices)}, Actual: {len(trainloader_1.dataset)}\")\n",
    "\n",
    "# Assign datasets based on clustering accuracy\n",
    "if overall_accuracy > 0.5:\n",
    "    fine_tuning_load = trainloader_1\n",
    "else:\n",
    "    fine_tuning_load = trainloader_0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "987a2c47c95a0ac3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#模型在测试集上的性能\n",
    "# 验证模型\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "# Ensure input data is on the same device during validation\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "865ca3975d9dc15",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Calculate model accuracy on specified dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained PyTorch model\n",
    "    - dataloader: DataLoader for evaluation\n",
    "    - device: Computation device ('cuda' or 'cpu')\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Accuracy score (float)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for faster inference\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3da4d907fe2dc32c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate model on cluster_0 and cluster_1 datasets\n",
    "accuracy_cluster_0 = evaluate_model(model, trainloader_0, device)\n",
    "accuracy_cluster_1 = evaluate_model(model, trainloader_1, device)\n",
    "accuracy_cluster = evaluate_model(model,trainloader,device)\n",
    "\n",
    "print(f\"Accuracy on Cluster 0: {accuracy_cluster_0:.2f}%\")\n",
    "print(f\"Accuracy on Cluster 1: {accuracy_cluster_1:.2f}%\")\n",
    "print(f\"Accuracy on Cluster : {accuracy_cluster:.2f}%\")\n",
    "\n",
    "accuracy_attack_train = evaluate_model(model, attackTrainloader , device)\n",
    "accuracy_clean_train = evaluate_model(model, cleanTrainloader , device)\n",
    "# accuracy_attack_test = evaluate_model(model, attackTestloader , device)\n",
    "accuracy_test = evaluate_model(model, testloader , device)\n",
    "\n",
    "print(f\"Accuracy on attack_train : {accuracy_attack_train:.2f}%\")\n",
    "print(f\"Accuracy on clean_train : {accuracy_clean_train:.2f}%\")\n",
    "\n",
    "print(f\"Accuracy on test : {accuracy_test:.2f}%\")\n",
    "accuracy_usps = evaluate_model(model, usps_loader , device)\n",
    "print(f\"Accuracy on usps_test : {accuracy_usps:.2f}%\")\n",
    "#print(f\"Accuracy on clean_test : {accuracy_clean_test:.2f}%\")\n",
    "accuracy_source = accuracy_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed6633462f7ad00b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(anchor, samples, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss (InfoNCE Loss)\n",
    "    :param anchor: Anchor vector (shape: [64])\n",
    "    :param samples: Sample matrix (shape: [n_samples, 64])\n",
    "    :param temperature: Temperature parameter\n",
    "    :return: Contrastive loss\n",
    "    \"\"\"\n",
    "    # Calculate cosine similarity between anchor and all samples\n",
    "    sim_matrix = F.cosine_similarity(anchor.unsqueeze(0), samples, dim=1) / temperature\n",
    "    # Contrastive loss calculation\n",
    "    loss = -torch.log(torch.exp(sim_matrix[0]) / torch.sum(torch.exp(sim_matrix)))\n",
    "    return loss\n",
    "\n",
    "def find_central_sample(cluster_attributions, batch_size=1024):\n",
    "    cluster_center = torch.mean(cluster_attributions, dim=0)\n",
    "    min_loss = float('inf')\n",
    "    best_sample = None\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(cluster_attributions), batch_size):\n",
    "        batch = cluster_attributions[i:i + batch_size]\n",
    "        losses = torch.stack([contrastive_loss(cluster_center, sample.unsqueeze(0)) for sample in batch])\n",
    "        min_batch_loss, min_batch_idx = torch.min(losses, dim=0)\n",
    "        if min_batch_loss < min_loss:\n",
    "            min_loss = min_batch_loss\n",
    "            best_sample = batch[min_batch_idx]\n",
    "\n",
    "    return best_sample\n",
    "\n",
    "# Example usage\n",
    "cluster_0_central_sample = find_central_sample(cluster_0_attributions)\n",
    "cluster_1_central_sample = find_central_sample(cluster_1_attributions)\n",
    "\n",
    "print(\"Cluster 0 的最中心样本:\", cluster_0_central_sample)\n",
    "print(\"Cluster 1 的最中心样本:\", cluster_1_central_sample)#也有好多02"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb74068a612d2d62",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "# Create a deep copy of the model\n",
    "model_copy = copy.deepcopy(model)\n",
    "\n",
    "# Ensure the new model is on the same device as the original\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy.to(device)\n",
    "\n",
    "# Verify successful copying\n",
    "print(model_copy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "295b330c45ea382e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_layer_features(model, data_loader, cluster_labels, layer_name='fc2'):\n",
    "    \"\"\"\n",
    "    Unified computation of model layer features and corresponding cluster labels\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        data_loader: Data loader (must return sample indices)\n",
    "        cluster_labels: Cluster label array (0/1)\n",
    "        layer_name: Target layer name ('fc1', 'fc2', or 'fc3')\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix of specified layer (n_samples, feature_dim)\n",
    "        y: Corresponding cluster labels (n_samples,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    # Ensure labels are numpy array\n",
    "    if isinstance(cluster_labels, torch.Tensor):\n",
    "        cluster_labels = cluster_labels.cpu().numpy()\n",
    "    \n",
    "    for images, indices in data_loader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass to specified layer\n",
    "        with torch.no_grad():\n",
    "            x = images.view(-1, 28*28)\n",
    "            \n",
    "            if layer_name == 'fc1':\n",
    "                features = torch.relu(model.fc1(x))\n",
    "            elif layer_name == 'fc2':\n",
    "                x = torch.relu(model.fc1(x))\n",
    "                features = torch.relu(model.fc2(x))\n",
    "            elif layer_name == 'fc3':\n",
    "                features = model(images)  # Full model output\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer name: {layer_name}\")\n",
    "        \n",
    "        # Store features and labels\n",
    "        X_list.append(features.cpu().numpy())\n",
    "        \n",
    "        # Get labels for current batch (handles indices automatically)\n",
    "        batch_labels = cluster_labels[indices.numpy()]\n",
    "        if overall_accuracy > 0.5:\n",
    "            inverse_labels =1 - batch_labels \n",
    "        else:\n",
    "            inverse_labels = batch_labels\n",
    "        y_list.append(inverse_labels)  # Label inversion\n",
    "    \n",
    "    # Concatenate results\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    \n",
    "    return X, y"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cb0be96bc967b7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# run func\n",
    "X_1,y_1 = compute_layer_features(model_copy, trainloader,gmm_labels,\"fc1\")\n",
    "X_2,y_2 = compute_layer_features(model_copy, trainloader,gmm_labels,\"fc2\")\n",
    "X_3,y_3 = compute_layer_features(model_copy, trainloader,gmm_labels,\"fc3\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb997a108a26af5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def solve_linear_regression_torch(X, y):\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Add bias term\n",
    "    ones = torch.ones(X.shape[0], 1)\n",
    "    X_bias = torch.cat([X, ones], dim=1)  # Shape becomes (60000, 65)\n",
    "\n",
    "    # Solve using least squares\n",
    "    W = torch.linalg.lstsq(X_bias, y).solution\n",
    "    return W[:-1], W[-1]  # W[:-1] are weights (n_features,), W[-1] is bias\n",
    "W2_torch, b2_torch = solve_linear_regression_torch(X_2, y_2)\n",
    "W1_torch, b1_torch = solve_linear_regression_torch(X_1, y_1)\n",
    "W3_torch, b3_torch = solve_linear_regression_torch(X_3, y_3)\n",
    "\n",
    "print(\"W shape:\", W2_torch.shape)  # (64,)\n",
    "print(\"b:\", b2_torch)\n",
    "# Print all weights\n",
    "for i, w_i in enumerate(W2_torch):\n",
    "    print(f\"W[{i}] = {w_i}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58f43bace120d95c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Flatten W_torch into a one-dimensional tensor\n",
    "W_torch = W2_torch.flatten()  # shape is (64,)\n",
    "\n",
    "# Sort in descending order by absolute value\n",
    "W_abs_sorted, indices = torch.sort(torch.abs(W_torch), descending=True)\n",
    "\n",
    "# Take the indices of the top 10 largest neurons\n",
    "top_10_indices = indices[:20]\n",
    "top_10_values = W_torch[top_10_indices]  # Get the corresponding W values\n",
    "\n",
    "# Print the contents of top_10_values\n",
    "print(\"Top 10 values:\", top_10_values)\n",
    "\n",
    "# output\n",
    "print(\"Top 10 neurons with highest absolute weights:\")\n",
    "for rank, (idx, val) in enumerate(zip(top_10_indices.tolist(), top_10_values.tolist()), start=1):\n",
    "    print(f\"Rank {rank}: Neuron {idx}, Weight = {float(val):.6f}\")  # Ensure val is a float How to prune\n",
    "# Get the weights and biases of the fc2 layer\n",
    "fc2_weight = model_copy.fc2.weight  # Weight matrix, shape (out_features, in_features)\n",
    "fc2_bias = model_copy.fc2.bias      # Bias vector, shape (out_features,)\n",
    "\n",
    "# Assume top_10_indices are the indices of the top 10 neurons obtained earlier\n",
    "#top_10_indices = torch.tensor([12, 45, 3, 28, 7, 19, 33, 56, 22, 41])  # Example data\n",
    "neurons_to_zero = top_10_indices[:20]  # Take the first 7 neurons\n",
    "print(\"Neurons to zero:\", neurons_to_zero)\n",
    "\n",
    "# Gradually set the weights and biases to 0\n",
    "for neuron_idx in neurons_to_zero:\n",
    "    # Set the corresponding neuron's weights to 0\n",
    "    fc2_weight.data[neuron_idx, :] = 0  # Set the weight row of this neuron to 0\n",
    "\n",
    "    # Set the corresponding neuron's bias to 0\n",
    "    #fc2_bias.data[neuron_idx] = 0  # Set the weight row of this neuron to 0\n",
    "\n",
    "    print(f\"Neuron {neuron_idx} weight and bias set to 0.\")\n",
    "\n",
    "# Print the modified weights and biases\n",
    "print(\"Modified fc2 weight:\")\n",
    "print(fc2_weight)\n",
    "\n",
    "print(\"Modified fc2 bias:\")\n",
    "print(fc2_bias)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e650096f4aa500ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Modify fc1 according to fc2 with 40 neurons\n",
    "# Flatten W_torch into a one-dimensional tensor\n",
    "W_torch = W1_torch.flatten()  # shape is (128,)\n",
    "\n",
    "# Sort in descending order by absolute value\n",
    "W_abs_sorted, indices = torch.sort(torch.abs(W_torch), descending=True)\n",
    "\n",
    "# Take the indices of the top 10 largest neurons\n",
    "top_10_indices = indices[:40]  # Take the first 10 indices 40 / 128\n",
    "top_10_values = W_torch[top_10_indices]  # Get the corresponding W values\n",
    "\n",
    "# Print the contents of top_10_values\n",
    "print(\"Top 10 values:\", top_10_values)\n",
    "\n",
    "# output\n",
    "print(\"Top 10 neurons with highest absolute weights:\")\n",
    "for rank, (idx, val) in enumerate(zip(top_10_indices.tolist(), top_10_values.tolist()), start=1):\n",
    "    print(f\"Rank {rank}: Neuron {idx}, Weight = {float(val):.6f}\")  # Ensure val is a float How to prune\n",
    "# Get the weights and biases of the fc2 layer\n",
    "fc1_weight = model_copy.fc.weight  # Weight matrix, shape (out_features, in_features)\n",
    "print(fc1_weight.shape)\n",
    "fc1_bias = model_copy.fc.bias      # Bias vector, shape (out_features,)\n",
    "\n",
    "# Assume top_10_indices are the indices of the top 10 neurons obtained earlier\n",
    "#top_10_indices = torch.tensor([12, 45, 3, 28, 7, 19, 33, 56, 22, 41])  # Example data\n",
    "neurons_to_zero = top_10_indices[:40]  # Take the first 7 neurons\n",
    "print(\"Neurons to zero:\", neurons_to_zero)\n",
    "\n",
    "# Gradually set the weights and biases to 0\n",
    "for neuron_idx in neurons_to_zero:\n",
    "    # Set the corresponding neuron's weights to 0\n",
    "    fc1_weight.data[neuron_idx, :] = 0  # Set the weight row of this neuron to 0\n",
    "\n",
    "    # Set the corresponding neuron's weights to 0\n",
    "    # fc2_bias.data[neuron_idx] = 0  # Set the weight row of this neuron to 0\n",
    "\n",
    "    print(f\"Neuron {neuron_idx} weight and bias set to 0.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f7c137ac6885b69",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Ensure the data is on the GPU (if available)\n",
    "# Test on the pruned model_copy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy.to(device)\n",
    "\n",
    "# Evaluate the model on the cluster_0 and cluster_1 datasets\n",
    "accuracy_cluster_0 = evaluate_model(model_copy, trainloader_0, device)\n",
    "accuracy_cluster_1 = evaluate_model(model_copy, trainloader_1, device)\n",
    "accuracy_cluster = evaluate_model(model_copy,trainloader,device)\n",
    "\n",
    "print(f\"Accuracy on Cluster 0: {accuracy_cluster_0:.2f}%\")\n",
    "print(f\"Accuracy on Cluster 1: {accuracy_cluster_1:.2f}%\")\n",
    "print(f\"Accuracy on Cluster : {accuracy_cluster:.2f}%\")\n",
    "\n",
    "accuracy_attack_train = evaluate_model(model_copy, attackTrainloader , device)\n",
    "accuracy_clean_train = evaluate_model(model_copy, cleanTrainloader , device)\n",
    "# accuracy_attack_test = evaluate_model(model, attackTestloader , device)\n",
    "accuracy_test = evaluate_model(model_copy, testloader , device)\n",
    "\n",
    "print(f\"Accuracy on attack_train : {accuracy_attack_train:.2f}%\")\n",
    "print(f\"Accuracy on clean_train : {accuracy_clean_train:.2f}%\")\n",
    "accuracy_prune = accuracy_test\n",
    "\n",
    "print(f\"Accuracy on test : {accuracy_test:.2f}%\")\n",
    "accuracy_usps = evaluate_model(model_copy, usps_loader , device)\n",
    "print(f\"Accuracy on usps_test : {accuracy_usps:.2f}%\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d165f89e3328e0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "# Deep copy the model\n",
    "model_copy_fine_all = copy.deepcopy(model_copy)\n",
    "\n",
    "# Ensure the new model is on the same device as the original\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy_fine_all.to(device)\n",
    "\n",
    "# Verify if the copy was successful\n",
    "print(model_copy_fine_all)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0f7d52125c7dc25",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Assume device is GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model_copy = model_copy.to(device)\n",
    "\n",
    "# Freeze all layers except fc2\n",
    "for name, param in model_copy.named_parameters():\n",
    "    if not name.startswith(\"fc2\") | name.startswith(\"fc1\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Check which layers' parameters are frozen\n",
    "for name, param in model_copy.named_parameters():\n",
    "    print(f\"{name}: requires_grad = {param.requires_grad}\")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model_copy.parameters(), lr=0.001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "import time\n",
    "\n",
    "# Training loop (with time statistics)\n",
    "num_epochs = 5\n",
    "total_training_time = 0  # Track total training time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()  # Record epoch start time\n",
    "    model_copy.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    batch_times = []  # Track time per batch\n",
    "\n",
    "    for inputs, labels in fine_tuning_load:\n",
    "        batch_start_time = time.time()  # Record batch start time\n",
    "\n",
    "        # Move input data and labels to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_copy(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Record batch time\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        batch_times.append(batch_time)\n",
    "\n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    total_training_time += epoch_time\n",
    "\n",
    "    # Calculate average batch time\n",
    "    avg_batch_time = sum(batch_times)/len(batch_times) if batch_times else 0\n",
    "\n",
    "    # Print training info (with time statistics)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Loss: {running_loss / len(trainloader):.4f}, \"\n",
    "          f\"Epoch Time: {epoch_time:.2f}s, \"\n",
    "          f\"Avg Batch Time: {avg_batch_time*1000:.1f}ms\")\n",
    "\n",
    "# Final time statistics\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
    "print(f\"Average epoch time: {total_training_time/num_epochs:.2f}s\")\n",
    "time_fc1_fc2 = total_training_time/num_epochs\n",
    "\n",
    "# Validate model\n",
    "model_copy.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for inputs, labels in testloader:\n",
    "        # Move input data and labels to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_copy(inputs)\n",
    "\n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Count correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56bb4c66e1d1966e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Assume device is GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model_copy_fine_all = model_copy_fine_all.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model_copy_fine_all.parameters(), lr=0.001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "import time\n",
    "\n",
    "# Training loop (with time tracking)\n",
    "num_epochs = 5\n",
    "total_training_time = 0  # Track total training time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()  # Record epoch start time\n",
    "    model_copy_fine_all.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    batch_times = []  # Track time per batch\n",
    "    \n",
    "    for inputs, labels in fine_tuning_load:\n",
    "        batch_start_time = time.time()  # Record batch start time\n",
    "\n",
    "        # Move inputs and labels to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_copy_fine_all(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Track batch time\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        batch_times.append(batch_time)\n",
    "\n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    total_training_time += epoch_time\n",
    "\n",
    "    # Calculate average batch time\n",
    "    avg_batch_time = sum(batch_times)/len(batch_times) if batch_times else 0\n",
    "\n",
    "    # Print training info (with timing)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Loss: {running_loss / len(trainloader):.4f}, \"\n",
    "          f\"Epoch Time: {epoch_time:.2f}s, \"\n",
    "          f\"Avg Batch Time: {avg_batch_time*1000:.1f}ms\")\n",
    "\n",
    "# Final timing statistics\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
    "print(f\"Average epoch time: {total_training_time/num_epochs:.2f}s\")\n",
    "time_all = total_training_time/num_epochs\n",
    "\n",
    "# Validate model\n",
    "model_copy_fine_all.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for inputs, labels in testloader:\n",
    "        # Move inputs and labels to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_copy_fine_all(inputs)\n",
    "\n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Count correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37b08a0b4174bff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Ensure data is on GPU (if available) - Testing on fine-tuned model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy.to(device)\n",
    "\n",
    "# Evaluate model on cluster_0 and cluster_1 datasets\n",
    "accuracy_cluster_0 = evaluate_model(model_copy, trainloader_0, device)\n",
    "accuracy_cluster_1 = evaluate_model(model_copy, trainloader_1, device)\n",
    "accuracy_cluster = evaluate_model(model_copy,trainloader,device)\n",
    "\n",
    "print(f\"Accuracy on Cluster 0: {accuracy_cluster_0:.2f}%\")\n",
    "print(f\"Accuracy on Cluster 1: {accuracy_cluster_1:.2f}%\")\n",
    "print(f\"Accuracy on Cluster : {accuracy_cluster:.2f}%\")\n",
    "\n",
    "accuracy_attack_train = evaluate_model(model_copy, attackTrainloader , device)\n",
    "accuracy_clean_train = evaluate_model(model_copy, cleanTrainloader , device)\n",
    "# accuracy_attack_test = evaluate_model(model, attackTestloader , device)\n",
    "accuracy_test = evaluate_model(model_copy, testloader , device)\n",
    "accuracy_test_all = evaluate_model(model_copy_fine_all, testloader , device)\n",
    "print(f\"Accuracy on attack_train : {accuracy_attack_train:.2f}%\")\n",
    "print(f\"Accuracy on clean_train : {accuracy_clean_train:.2f}%\")\n",
    "#print(f\"Accuracy on test souse : {accuracy_test:.2f}%\")\n",
    "\n",
    "print(f\"Average epoch time: {time_source:.2f}s\")\n",
    "print(f\"Accuracy on test : {accuracy_source:.2f}%\")\n",
    "print(f\"Accuracy on test_prune : {accuracy_prune:.2f}%\")\n",
    "print(f\"Average epoch time on special: {time_fc1_fc2:.2f}s\")\n",
    "print(f\"Accuracy on test_fin-tuning special : {accuracy_test:.2f}%\")\n",
    "print(f\"Average epoch time on all: {time_all:.2f}s\")\n",
    "print(f\"Accuracy on test_fin-tuning all : {accuracy_test_all:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7776dcac1e20d9e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
