{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Hyperparameter\n",
    "DATA_DIR      = 'speech_commands'\n",
    "BATCH_SIZE    = 64\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "NUM_CLASSES   = 10\n",
    "SAMPLE_RATE   = 16000\n",
    "DURATION      = 1\n",
    "N_MFCC        = 40\n",
    "COMMANDS      = ['yes','no','up','down','left','right','on','off','stop','go']\n",
    "\n",
    "trainepochs = 300\n",
    "fineepochs = 150\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1) Load and extract MFCC features\n",
    "def load_audio_files(parent_dir, sub_dirs, target_length=130):\n",
    "    features, labels = [], []\n",
    "    for label, sub in enumerate(sub_dirs):\n",
    "        folder = os.path.join(parent_dir, sub)\n",
    "        for fn in os.listdir(folder):\n",
    "            if not fn.endswith('.wav'): continue\n",
    "            path = os.path.join(folder, fn)\n",
    "            audio, _ = librosa.load(path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "            mfccs = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=N_MFCC)\n",
    "            # Pad/truncate to fixed length\n",
    "            if mfccs.shape[1] < target_length:\n",
    "                pad = target_length - mfccs.shape[1]\n",
    "                mfccs = np.pad(mfccs, ((0,0),(0,pad)), 'constant')\n",
    "            else:\n",
    "                mfccs = mfccs[:, :target_length]\n",
    "            features.append(mfccs)\n",
    "            labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "features, labels = load_audio_files(DATA_DIR, COMMANDS)\n",
    "print(\"Loaded features:\", features.shape, \"labels:\", labels.shape)\n",
    "\n",
    "# 2) Stratified train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# 3) Define base Dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels   = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.features[idx])    # [N_MFCC, T]\n",
    "        y = torch.LongTensor([self.labels[idx]])     # [1]\n",
    "        return x, y\n",
    "\n",
    "trainset = AudioDataset(X_train, y_train)\n",
    "testset  = AudioDataset(X_test, y_test)\n",
    "\n",
    "# 4) Randomly select 50% samples as \"attacked\" samples\n",
    "num_train_attack     = len(trainset) // 2\n",
    "attack_train_indices = set(random.sample(range(len(trainset)), num_train_attack))\n",
    "train_clean_indices  = set(range(len(trainset))) - attack_train_indices\n",
    "\n",
    "print(f\"Train Clean Samples: {len(train_clean_indices)}, Attacked Samples: {len(attack_train_indices)}\")\n",
    "\n",
    "# 5) Gaussian noise function & NoisyDataset\n",
    "def apply_audio_noise(mfcc_tensor, noise_level=3):\n",
    "    noise = torch.randn_like(mfcc_tensor) * noise_level\n",
    "    return mfcc_tensor + noise\n",
    "\n",
    "class NoisyAudioDataset(Dataset):\n",
    "    def __init__(self, dataset, attacked_indices, noise_level=0.01):\n",
    "        self.dataset        = dataset\n",
    "        self.attacked_indices = attacked_indices\n",
    "        self.noise_level    = noise_level\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc, label = self.dataset[idx]\n",
    "        if idx in self.attacked_indices:\n",
    "            mfcc = apply_audio_noise(mfcc, self.noise_level)\n",
    "        return mfcc, label\n",
    "\n",
    "# 6) Create noisy training set and DataLoaders\n",
    "trainset_noisy    = NoisyAudioDataset(trainset, attack_train_indices, noise_level=15)\n",
    "trainloader       = DataLoader(trainset_noisy, batch_size=BATCH_SIZE, shuffle=False)\n",
    "testloader        = DataLoader(testset,        batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 7) Create subsets for attacked and clean samples\n",
    "train_attack_subset = Subset(trainset_noisy, list(attack_train_indices))\n",
    "train_clean_subset  = Subset(trainset_noisy, list(train_clean_indices))\n",
    "\n",
    "attackTrainloader   = DataLoader(train_attack_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "cleanTrainloader    = DataLoader(train_clean_subset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"attackTrain samples: {len(train_attack_subset)}\")\n",
    "print(f\"cleanTrain samples : {len(train_clean_subset)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "class ImprovedAudioCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ImprovedAudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 2 * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = ImprovedAudioCNN(NUM_CLASSES).to(device)\n",
    "    summary(model, (N_MFCC, features[0].shape[1]))  # Input shape: (N_MFCC, time_steps)\n",
    "\n",
    "    # Optimizer with learning rate scheduling\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "    best_test_acc = 0\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    epoch_times = []  # New: for storing each epoch's duration\n",
    "\n",
    "    for epoch in range(trainepochs):\n",
    "        start_time = time.time()  # Record start time\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.squeeze().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Evaluation on test set\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(device), labels.squeeze().to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "\n",
    "        end_time = time.time()  # Record end time\n",
    "        epoch_time = end_time - start_time  # Calculate epoch duration\n",
    "        epoch_times.append(epoch_time)  # Store epoch time\n",
    "\n",
    "        train_loss_avg = train_loss / len(trainloader)\n",
    "        train_acc = train_correct / len(trainset)\n",
    "        test_acc = test_correct / test_total\n",
    "\n",
    "        train_losses.append(train_loss_avg)\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step(test_acc)\n",
    "\n",
    "        print(f'\\nEpoch {epoch+1}/{trainepochs}:')\n",
    "        print(f'Train Loss: {train_loss_avg:.4f} | Train Acc: {train_acc:.4f}')\n",
    "        print(f'Test Acc : {test_acc:.4f}')\n",
    "        print(f'Epoch Time: {epoch_time:.2f} seconds')  # Print epoch duration\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print('Best model saved!')\n",
    "\n",
    "    # Calculate average epoch time\n",
    "    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(test_accs, label='Test Acc')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Output statistics\n",
    "    print(f'\\nTraining completed!')\n",
    "    print(f'Total training time: {sum(epoch_times):.2f} seconds')\n",
    "    print(f'Average epoch time: {avg_epoch_time:.2f} seconds')\n",
    "    print(f'Best Test Accuracy: {best_test_acc:.4f}')\n",
    "    torch.save(model, 'final_model.pth')\n",
    "    print('Final model saved!')"
   ],
   "id": "ce5c3da48e769e60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_ffn_attribution(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Compute attribution for the first fully-connected layer (Linear(64*2*2, 128)) in ImprovedAudioCNN\n",
    "\n",
    "    Args:\n",
    "        model: Trained ImprovedAudioCNN model\n",
    "        data_loader: Audio data loader returning (mfcc, label) tuples\n",
    "        device: 'cuda' or 'cpu'\n",
    "    Returns:\n",
    "        attributions: Attribution matrix of shape (num_samples, 128)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    attributions = []\n",
    "\n",
    "    for mfccs, _ in data_loader:\n",
    "        mfccs = mfccs.to(device)\n",
    "        mfccs.requires_grad_()\n",
    "\n",
    "        # ===== Manual forward pass up to fc1 =====\n",
    "        x = mfccs.unsqueeze(1)            # (B,1,N_MFCC,T)\n",
    "        x = model.conv1(x)                # (B,64,*,*)\n",
    "        x = model.adaptive_pool(x)        # (B,64,2,2)\n",
    "        x = x.view(x.size(0), -1)         # (B, 64*2*2)\n",
    "\n",
    "        # Save input to first fully-connected layer (fc1)\n",
    "        fc_input = x.clone()\n",
    "\n",
    "        # First FC layer + activation\n",
    "        z1 = model.fc[0](x)               # (B,128)\n",
    "        a1 = F.relu(z1)                   # (B,128)\n",
    "\n",
    "        # Second FC layer to get final logits\n",
    "        logits = model.fc[2](a1)          # (B, num_classes)\n",
    "\n",
    "        # For each class, compute gradient of logits[:, i] w.r.t z1\n",
    "        grad_list = []\n",
    "        for i in range(logits.size(1)):\n",
    "            model.zero_grad()\n",
    "            grads = torch.autograd.grad(\n",
    "                outputs=logits[:, i],\n",
    "                inputs=z1,\n",
    "                grad_outputs=torch.ones_like(logits[:, i]),\n",
    "                retain_graph=True,\n",
    "                create_graph=False\n",
    "            )[0]                         # (B,128)\n",
    "            grad_list.append(grads)\n",
    "\n",
    "        # Stack into (B, num_classes, 128)\n",
    "        grads_all = torch.stack(grad_list, dim=1)\n",
    "\n",
    "        # Take max response across classes, multiply by activation a1, and take absolute value\n",
    "        # Result shape: (B,128)\n",
    "        attr = (grads_all.max(dim=1)[0] * a1).abs()\n",
    "\n",
    "        attributions.append(attr.detach().cpu())\n",
    "\n",
    "    return torch.cat(attributions, dim=0)  # (total_samples, 128)"
   ],
   "id": "ccaf97f2aa8e5aef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Automatically select device\n",
    "model.to(device)  # Move model to GPU\n",
    "# Compute attribution\n",
    "attributions = compute_ffn_attribution(model, trainloader,device)\n",
    "\n",
    "# attributions shape is (num_samples, 256), representing each sample's importance on 256-dimensional features"
   ],
   "id": "bd624f240f3996e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "attributions.shape"
   ],
   "id": "31ac75a8f44eb9aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume ffn_attributions is a PyTorch tensor on GPU\n",
    "# 1. Move tensor from GPU to CPU\n",
    "ffn_attributions_cpu = attributions.cpu()\n",
    "\n",
    "# 2. Convert PyTorch tensor to NumPy array\n",
    "ffn_attributions_np = ffn_attributions_cpu.numpy()\n",
    "\n",
    "# 3. Standardize using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(ffn_attributions_np)\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# GMM clustering\n",
    "gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(X_scaled)\n",
    "\n",
    "# Get indices for two clusters\n",
    "cluster_0_indices = np.where(gmm_labels == 0)[0]\n",
    "cluster_1_indices = np.where(gmm_labels == 1)[0]\n",
    "\n",
    "print(f\"\\nGMM分类结果:\")\n",
    "print(f\"Cluster 0 样本数: {len(cluster_0_indices)}\")\n",
    "print(f\"Cluster 1 样本数: {len(cluster_1_indices)}\")"
   ],
   "id": "756a6229fd168dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the matching between K-Means clustering and manual partitioning\n",
    "attack_set = set(attack_train_indices)  # True attack sample indices\n",
    "clean_set = set(train_clean_indices)    # True clean sample indices\n",
    "\n",
    "cluster_0_set = set(cluster_0_indices)  # Samples in K-Means cluster 0\n",
    "cluster_1_set = set(cluster_1_indices)  # Samples in K-Means cluster 1\n",
    "\n",
    "# Calculate intersections\n",
    "attack_in_cluster_0 = len(attack_set & cluster_0_set)  # Number of true attack samples in Cluster 0\n",
    "attack_in_cluster_1 = len(attack_set & cluster_1_set)  # Number of true attack samples in Cluster 1\n",
    "\n",
    "clean_in_cluster_0 = len(clean_set & cluster_0_set)  # Number of true clean samples in Cluster 0\n",
    "clean_in_cluster_1 = len(clean_set & cluster_1_set)  # Number of true clean samples in Cluster 1\n",
    "\n",
    "# Calculate classification accuracy of K-Means for attack samples\n",
    "attack_accuracy_cluster_0 = (attack_in_cluster_0 / len(attack_set)) * 100\n",
    "attack_accuracy_cluster_1 = (attack_in_cluster_1 / len(attack_set)) * 100\n",
    "\n",
    "clean_accuracy_cluster_0 = (clean_in_cluster_0 / len(clean_set)) * 100\n",
    "clean_accuracy_cluster_1 = (clean_in_cluster_1 / len(clean_set)) * 100\n",
    "\n",
    "# Print comparison results\n",
    "print(\"==== K-Means Clustering vs Manual Labels ====\")\n",
    "print(f\"Cluster 0: {len(cluster_0_indices)} samples\")\n",
    "print(f\"Cluster 1: {len(cluster_1_indices)} samples\\n\")\n",
    "\n",
    "print(f\"Attack Samples in Cluster 0: {attack_in_cluster_0} ({attack_accuracy_cluster_0:.2f}%)\")\n",
    "print(f\"Attack Samples in Cluster 1: {attack_in_cluster_1} ({attack_accuracy_cluster_1:.2f}%)\\n\")\n",
    "\n",
    "print(f\"Clean Samples in Cluster 0: {clean_in_cluster_0} ({clean_accuracy_cluster_0:.2f}%)\")\n",
    "print(f\"Clean Samples in Cluster 1: {clean_in_cluster_1} ({clean_accuracy_cluster_1:.2f}%)\")\n",
    "\n",
    "# Calculate overall classification accuracy of K-Means\n",
    "total_correct = attack_in_cluster_0 + clean_in_cluster_1  # Assuming cluster_0 mainly contains attack samples, cluster_1 mainly clean samples\n",
    "overall_accuracy = (total_correct / len(trainset)) * 100  # Previously train_subset, now changed to trainset\n",
    "\n",
    "print(f\"\\nOverall Clustering Accuracy: {overall_accuracy:.2f}%\")"
   ],
   "id": "427c34237309ff39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract samples from cluster 0\n",
    "cluster_0_attributions = attributions[cluster_0_indices]\n",
    "\n",
    "# Extract samples from cluster 1\n",
    "cluster_1_attributions = attributions[cluster_1_indices]\n",
    "print(cluster_0_attributions.shape)\n",
    "print(cluster_1_attributions.shape)"
   ],
   "id": "5fe6baf206cd2362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Ensure labels length matches dataset size gam_labels\n",
    "# assert len(labels) == len(trainloader.dataset), \"labels and trainloader.dataset size mismatch!\"\n",
    "\n",
    "# Convert indices to Python list\n",
    "cluster_0_indices = cluster_0_indices.tolist()\n",
    "cluster_1_indices = cluster_1_indices.tolist()\n",
    "\n",
    "# Use Subset to split original dataset by indices\n",
    "dataset_0 = Subset(trainloader.dataset, cluster_0_indices)\n",
    "dataset_1 = Subset(trainloader.dataset, cluster_1_indices)\n",
    "\n",
    "# Create new DataLoaders\n",
    "trainloader_0 = DataLoader(dataset_0, batch_size=64, shuffle=True)\n",
    "trainloader_1 = DataLoader(dataset_1, batch_size=64, shuffle=True)\n",
    "\n",
    "# Final verification\n",
    "print(f\"Expected Cluster 0 size: {len(cluster_0_indices)}, Actual: {len(trainloader_0.dataset)}\")\n",
    "print(f\"Expected Cluster 1 size: {len(cluster_1_indices)}, Actual: {len(trainloader_1.dataset)}\")\n",
    "\n",
    "if overall_accuracy > 0.5:\n",
    "    fine_tuning_load = trainloader_1\n",
    "else:\n",
    "    fine_tuning_load = trainloader_0\n"
   ],
   "id": "df550f9841439a58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Calculate the model's accuracy on a given dataset.\n",
    "\n",
    "    Args:\n",
    "      - model: Trained PyTorch model\n",
    "      - dataloader: DataLoader for evaluation\n",
    "      - device: Computation device ('cuda' or 'cpu')\n",
    "\n",
    "    Returns:\n",
    "      - accuracy: Accuracy score (float)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            # Flatten labels to shape (B,)\n",
    "            labels = labels.to(device).view(-1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model_with_metric(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on a given dataset, including Accuracy, Precision, F1 and Top-3 Accuracy.\n",
    "\n",
    "    Args:\n",
    "      - model: Trained PyTorch model\n",
    "      - dataloader: Test DataLoader\n",
    "      - device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "      - results: Dictionary containing accuracy, precision, f1, top3_accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    top3_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).view(-1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Top-1 predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Top-3 predictions\n",
    "            top3 = torch.topk(outputs, k=3, dim=1).indices  # shape: [B, 3]\n",
    "            top3_correct += sum([labels[i].item() in top3[i] for i in range(len(labels))])\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100.0 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0) * 100\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0) * 100\n",
    "    top3_accuracy = 100.0 * top3_correct / total\n",
    "\n",
    "    results = {\n",
    "        'Accuracy (%)': accuracy,\n",
    "        'Precision (macro, %)' : precision,\n",
    "        'F1-score (macro, %)' : f1,\n",
    "        'Top-3 Accuracy (%)': top3_accuracy\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n"
   ],
   "id": "98ac67190565ce2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate model on cluster_0 and cluster_1 datasets\n",
    "accuracy_cluster_0 = evaluate_model(model, trainloader_0, device)\n",
    "accuracy_cluster_1 = evaluate_model(model, trainloader_1, device)\n",
    "accuracy_cluster = evaluate_model(model,trainloader,device)\n",
    "\n",
    "print(f\"Accuracy on Cluster 0: {accuracy_cluster_0:.2f}%\")\n",
    "print(f\"Accuracy on Cluster 1: {accuracy_cluster_1:.2f}%\")\n",
    "print(f\"Accuracy on Cluster : {accuracy_cluster:.2f}%\")\n",
    "\n",
    "accuracy_attack_train = evaluate_model(model, attackTrainloader , device)\n",
    "accuracy_clean_train = evaluate_model(model, cleanTrainloader , device)\n",
    "# accuracy_attack_test = evaluate_model(model, attackTestloader , device)\n",
    "accuracy_test = evaluate_model(model, testloader , device)\n",
    "\n",
    "print(f\"Accuracy on attack_train : {accuracy_attack_train:.2f}%\")\n",
    "print(f\"Accuracy on clean_train : {accuracy_clean_train:.2f}%\")\n",
    "\n",
    "print(f\"Accuracy on test : {accuracy_test:.2f}%\")\n",
    "\n",
    "#print(f\"Accuracy on clean_test : {accuracy_clean_test:.2f}%\")\n",
    "accuracy_source = accuracy_test\n",
    "\n",
    "metric_test = evaluate_model_with_metric(model, testloader , device)\n",
    "print(f\"Accuracy  : {metric_test['Accuracy (%)']:.2f}%\")\n",
    "print(f\"Precision (macro  : {metric_test['Precision (macro, %)']:.2f}%\")\n",
    "print(f\"F1-score (macro, %)  : {metric_test['F1-score (macro, %)']:.2f}%\")\n",
    "print(f\"Top-3 Accuracy (%)  : {metric_test['Top-3 Accuracy (%)']:.2f}%\")"
   ],
   "id": "81a82e62a19a511b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_layer_features(model, data_loader, cluster_labels=None,\n",
    "                           layer_name='fc1', overall_accuracy=1.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Extract features from specified fully-connected layer in ImprovedAudioCNN\n",
    "\n",
    "    Args:\n",
    "        model: Trained ImprovedAudioCNN model\n",
    "        data_loader: DataLoader returning (mfcc, label) tuples\n",
    "        cluster_labels: Optional array of cluster labels (0 or 1) with length equal to total samples\n",
    "        layer_name: Currently only supports 'fc1' (corresponding to first Linear(256,128) layer\n",
    "        overall_accuracy: Current model accuracy used to determine label inversion\n",
    "        device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix (n_samples, 128)\n",
    "        y: Label array if cluster_labels provided, else None\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    if cluster_labels is not None and isinstance(cluster_labels, torch.Tensor):\n",
    "        cluster_labels = cluster_labels.cpu().numpy()\n",
    "\n",
    "    batch_size = data_loader.batch_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (mfccs, _) in enumerate(data_loader):\n",
    "            # Move inputs to device\n",
    "            mfccs = mfccs.to(device)\n",
    "\n",
    "            # Forward pass to fc1 input\n",
    "            x = mfccs.unsqueeze(1)           # (B,1,N_MFCC,T)\n",
    "            x = model.conv1(x)               # (B,64,*,*)\n",
    "            x = model.adaptive_pool(x)       # (B,64,2,2)\n",
    "            x = x.view(x.size(0), -1)        # (B,256)\n",
    "\n",
    "            if layer_name != 'fc1':\n",
    "                raise ValueError(f\"Unsupported layer name: {layer_name}. Only 'fc1' is supported.\")\n",
    "\n",
    "           # Extract fc1 layer features\n",
    "            features = model.fc[0](x)        # (B,128)\n",
    "\n",
    "            X_list.append(features.cpu().numpy())\n",
    "\n",
    "            if cluster_labels is not None:\n",
    "                start = batch_idx * batch_size\n",
    "                end   = start + features.size(0)\n",
    "                batch_labels = cluster_labels[start:end]\n",
    "                if overall_accuracy > 0.5:\n",
    "                    batch_labels = 1 - batch_labels\n",
    "                y_list.append(batch_labels)\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0) if cluster_labels is not None else None\n",
    "    return X, y\n"
   ],
   "id": "f7c002170083e842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "# Deep copy the model\n",
    "model_copy = copy.deepcopy(model)\n",
    "\n",
    "# Ensure the new model is on the same device as the original\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy.to(device)\n",
    "\n",
    "# Verify if the copy was successful\n",
    "print(model_copy)\n",
    "#操作model-copy version2"
   ],
   "id": "8834903edb98010",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_1,y_1 = compute_layer_features(model_copy, trainloader,gmm_labels)"
   ],
   "id": "68c376ed8969700c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def solve_linear_regression_torch(X, y):\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Add bias term\n",
    "    ones = torch.ones(X.shape[0], 1)\n",
    "    X_bias = torch.cat([X, ones], dim=1)  # Shape becomes (60000, 65)\n",
    "\n",
    "    # Solve using least squares\n",
    "    W = torch.linalg.lstsq(X_bias, y).solution\n",
    "    return W[:-1], W[-1]  # W[:-1] are weights (n_features,), W[-1] is bias\n",
    "# W2_torch, b2_torch = solve_linear_regression_torch(X_2, y_2)\n",
    "W1_torch, b1_torch = solve_linear_regression_torch(X_1, y_1)\n",
    "\n",
    "print(\"W1 shape:\", W1_torch.shape)  # (64,)\n",
    "print(\"b:\", b1_torch)\n",
    "# print all weights\n",
    "# for i, w_i in enumerate(W2_torch):\n",
    "#     print(f\"W[{i}] = {w_i}\")"
   ],
   "id": "61c09719bc422851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "W_torch = W1_torch.flatten()  # shape is (128,)\n",
    "\n",
    "# Sort in descending order by absolute value\n",
    "W_abs_sorted, indices = torch.sort(torch.abs(W_torch), descending=True)\n",
    "\n",
    "# Take the indices of the top 10 largest neurons\n",
    "top_10_indices = indices[:20]\n",
    "top_10_values = W_torch[top_10_indices]  # Get the corresponding W values\n",
    "\n",
    "# Print the contents of top_10_values\n",
    "print(\"Top 10 values:\", top_10_values)\n",
    "\n",
    "# output\n",
    "print(\"Top 10 neurons with highest absolute weights:\")\n",
    "for rank, (idx, val) in enumerate(zip(top_10_indices.tolist(), top_10_values.tolist()), start=1):\n",
    "    print(f\"Rank {rank}: Neuron {idx}, Weight = {float(val):.6f}\")  # Ensure val is a float How to prune\n",
    "# Get the weights and biases of the fc2 layer\n",
    "fc1_weight = model_copy.fc[0].weight  # Weight matrix, shape (out_features, in_features)\n",
    "fc1_bias = model_copy.fc[0].bias      # Bias vector, shape (out_features,)\n",
    "\n",
    "# Assume top_10_indices are the indices of the top 10 neurons obtained earlier\n",
    "#top_10_indices = torch.tensor([12, 45, 3, 28, 7, 19, 33, 56, 22, 41])  # Example data\n",
    "neurons_to_zero = top_10_indices[:40]  # Take the first 7 neurons\n",
    "print(\"Neurons to zero:\", neurons_to_zero)\n",
    "\n",
    "# Gradually set the weights and biases to 0\n",
    "for neuron_idx in neurons_to_zero:\n",
    "    # Set the corresponding neuron's weights to 0\n",
    "    fc1_weight.data[neuron_idx, :] = 0  # Set the weight row of this neuron to 0\n",
    "    # Set the corresponding neuron's weights to 0\n",
    "    #fc2_bias.data[neuron_idx] = 0  # Set the weight row of this neuron to 0\n",
    "    print(f\"Neuron {neuron_idx} weight and bias set to 0.\")"
   ],
   "id": "ebf60c04e74c00b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prune testing\n",
    "# Ensure the data is on the GPU (if available)\n",
    "# Test on the pruned model_copy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy.to(device)\n",
    "\n",
    "# Evaluate the model on the cluster_0 and cluster_1 datasets\n",
    "accuracy_cluster_0 = evaluate_model(model_copy, trainloader_0, device)\n",
    "accuracy_cluster_1 = evaluate_model(model_copy, trainloader_1, device)\n",
    "accuracy_cluster = evaluate_model(model_copy,trainloader,device)\n",
    "\n",
    "print(f\"Accuracy on Cluster 0: {accuracy_cluster_0:.2f}%\")\n",
    "print(f\"Accuracy on Cluster 1: {accuracy_cluster_1:.2f}%\")\n",
    "print(f\"Accuracy on Cluster : {accuracy_cluster:.2f}%\")\n",
    "\n",
    "accuracy_attack_train = evaluate_model(model_copy, attackTrainloader , device)\n",
    "accuracy_clean_train = evaluate_model(model_copy, cleanTrainloader , device)\n",
    "# accuracy_attack_test = evaluate_model(model, attackTestloader , device)\n",
    "accuracy_test = evaluate_model(model_copy, testloader , device)\n",
    "\n",
    "print(f\"Accuracy on attack_train : {accuracy_attack_train:.2f}%\")\n",
    "print(f\"Accuracy on clean_train : {accuracy_clean_train:.2f}%\")\n",
    "accuracy_prune = accuracy_test\n",
    "\n",
    "print(f\"Accuracy on test : {accuracy_test:.2f}%\")\n",
    "metric_prune = evaluate_model_with_metric(model_copy, testloader , device)\n",
    "\n",
    "print(f\"Accuracy  : {metric_prune['Accuracy (%)']:.2f}%\")\n",
    "print(f\"Precision (macro  : {metric_prune['Precision (macro, %)']:.2f}%\")\n",
    "print(f\"F1-score (macro, %)  : {metric_prune['F1-score (macro, %)']:.2f}%\")\n",
    "print(f\"Top-3 Accuracy (%)  : {metric_prune['Top-3 Accuracy (%)']:.2f}%\")"
   ],
   "id": "d323954c6ad0fbb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "# Deep copy the model\n",
    "model_copy_fine_all = copy.deepcopy(model_copy)\n",
    "\n",
    "# Ensure the new model is on the same device as the original\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy_fine_all.to(device)\n",
    "\n",
    "# Verify if the copy was successful\n",
    "print(model_copy_fine_all)\n",
    "# Preparation for fine-tuning"
   ],
   "id": "806f4b0e4a7712",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assume device is GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model_copy = model_copy.to(device)\n",
    "\n",
    "# Freeze all layers except fc.0\n",
    "for name, param in model_copy.named_parameters():\n",
    "    param.requires_grad = name.startswith(\"fc.0.\")\n",
    "\n",
    "# Check which layers' parameters are frozen\n",
    "print(\"\\n=== requires_grad 状态 ===\")\n",
    "for name, param in model_copy.named_parameters():\n",
    "    print(f\"{name}: requires_grad = {param.requires_grad}\")\n",
    "\n",
    "# Define optimizer - only optimize parameters that require gradients\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model_copy.parameters()),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-5  # Add weight decay to prevent overfitting\n",
    ")\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "import time\n",
    "\n",
    "# Training loop (with time statistics)\n",
    "total_training_time = 0\n",
    "\n",
    "for epoch in range(fineepochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model_copy.train()\n",
    "    running_loss = 0.0\n",
    "    batch_times = []\n",
    "\n",
    "    for inputs, labels in fine_tuning_load:\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # 移动到设备并展平 labels Move data and labels to device and flatten\n",
    "        inputs, labels = inputs.to(device), labels.to(device).view(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_copy(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        batch_times.append(time.time() - batch_start_time)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    total_training_time += epoch_time\n",
    "    avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{fineepochs}], \"\n",
    "        f\"Loss: {running_loss / len(fine_tuning_load):.4f}, \"\n",
    "        f\"Epoch Time: {epoch_time:.2f}s, \"\n",
    "        f\"Avg Batch Time: {avg_batch_time*1000:.1f}ms\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
    "print(f\"Average epoch time: {total_training_time/fineepochs:.2f}s\")\n",
    "time_fc1_fc2 = total_training_time / fineepochs\n",
    "\n",
    "# Validate model\n",
    "model_copy.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).view(-1)\n",
    "        outputs = model_copy(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n"
   ],
   "id": "77ef2e7d7046b51b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assume device is GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model_copy_fine_all = model_copy_fine_all.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model_copy_fine_all.parameters(), lr=0.001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "import time\n",
    "\n",
    "# Training loop (with time tracking)\n",
    "total_training_time = 0  # Track total training time\n",
    "\n",
    "for epoch in range(fineepochs):\n",
    "    epoch_start_time = time.time()  # Record epoch start time\n",
    "    model_copy_fine_all.train()     # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    batch_times = []                # Track time per batch\n",
    "\n",
    "    for inputs, labels in fine_tuning_load:\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # Move inputs and labels to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device).view(-1)\n",
    "\n",
    "        optimizer.zero_grad()       # Zero gradients\n",
    "\n",
    "        outputs = model_copy_fine_all(inputs)    # Forward\n",
    "        loss = criterion(outputs, labels)        # Compute loss\n",
    "        loss.backward()                          # Backward\n",
    "        optimizer.step()                         # Update  parameters\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        batch_times.append(time.time() - batch_start_time)\n",
    "\n",
    "    # Calculate time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    total_training_time += epoch_time\n",
    "    avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{fineepochs}], \"\n",
    "        f\"Loss: {running_loss / len(fine_tuning_load):.4f}, \"\n",
    "        f\"Epoch Time: {epoch_time:.2f}s, \"\n",
    "        f\"Avg Batch Time: {avg_batch_time*1000:.1f}ms\"\n",
    "    )\n",
    "\n",
    "# Final timing statistics\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
    "print(f\"Average epoch time: {total_training_time/fineepochs:.2f}s\")\n",
    "time_all = total_training_time / fineepochs\n",
    "\n",
    "# Validate model\n",
    "model_copy_fine_all.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).view(-1)  # 展平 labels\n",
    "        outputs = model_copy_fine_all(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n"
   ],
   "id": "b5c5ddd264d8777a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure data is on GPU (if available) - Testing on fine-tuned model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy.to(device)\n",
    "\n",
    "# Evaluate model on cluster_0 and cluster_1 datasets\n",
    "accuracy_cluster_0 = evaluate_model(model_copy, trainloader_0, device)\n",
    "accuracy_cluster_1 = evaluate_model(model_copy, trainloader_1, device)\n",
    "accuracy_cluster = evaluate_model(model_copy,trainloader,device)\n",
    "\n",
    "print(f\"Accuracy on Cluster 0: {accuracy_cluster_0:.2f}%\")\n",
    "print(f\"Accuracy on Cluster 1: {accuracy_cluster_1:.2f}%\")\n",
    "print(f\"Accuracy on Cluster : {accuracy_cluster:.2f}%\")\n",
    "\n",
    "accuracy_attack_train = evaluate_model(model_copy, attackTrainloader , device)\n",
    "accuracy_clean_train = evaluate_model(model_copy, cleanTrainloader , device)\n",
    "# accuracy_attack_test = evaluate_model(model, attackTestloader , device)\n",
    "accuracy_test = evaluate_model(model_copy, testloader , device)\n",
    "accuracy_test_all = evaluate_model(model_copy_fine_all, testloader , device)\n",
    "print(f\"Accuracy on attack_train : {accuracy_attack_train:.2f}%\")\n",
    "print(f\"Accuracy on clean_train : {accuracy_clean_train:.2f}%\")\n",
    "#print(f\"Accuracy on test souse : {accuracy_test:.2f}%\")\n",
    "\n",
    "print(f\"Accuracy on test : {accuracy_source:.2f}%\")\n",
    "print(f\"Accuracy on test_prune : {accuracy_prune:.2f}%\")\n",
    "print(f\"Average epoch time on special: {time_fc1_fc2:.2f}s\")\n",
    "print(f\"Accuracy on test_fin-tuning special : {accuracy_test:.2f}%\")\n",
    "print(f\"Average epoch time on all: {time_all:.2f}s\")\n",
    "print(f\"Accuracy on test_fin-tuning all : {accuracy_test_all:.2f}%\")\n",
    "\n",
    "metric_sigle = evaluate_model_with_metric(model_copy, testloader , device)\n",
    "metric_All = evaluate_model_with_metric(model_copy_fine_all, testloader , device)\n",
    "print(\"singel\")\n",
    "print(f\"Accuracy  : {metric_sigle['Accuracy (%)']:.2f}%\")\n",
    "print(f\"Precision (macro  : {metric_sigle['Precision (macro, %)']:.2f}%\")\n",
    "print(f\"F1-score (macro, %)  : {metric_sigle['F1-score (macro, %)']:.2f}%\")\n",
    "print(f\"Top-3 Accuracy (%)  : {metric_sigle['Top-3 Accuracy (%)']:.2f}%\")\n",
    "print(\"ALL\")\n",
    "print(f\"Accuracy  : {metric_All['Accuracy (%)']:.2f}%\")\n",
    "print(f\"Precision (macro  : {metric_All['Precision (macro, %)']:.2f}%\")\n",
    "print(f\"F1-score (macro, %)  : {metric_All['F1-score (macro, %)']:.2f}%\")\n",
    "print(f\"Top-3 Accuracy (%)  : {metric_All['Top-3 Accuracy (%)']:.2f}%\")\n"
   ],
   "id": "4152556ac0ac9c47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_retrain = ImprovedAudioCNN(NUM_CLASSES).to(device)\n",
    "summary(model, (N_MFCC, features[0].shape[1]))  # Input shape: (N_MFCC, time_steps)\n",
    "\n",
    "# Using an optimizer with learning rate degradation\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_retrain.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "best_test_acc = 0\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "etime1 = time.time()\n",
    "for epoch in range(trainepochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "\n",
    "    for inputs, labels in fine_tuning_load:\n",
    "        inputs, labels = inputs.to(device), labels.squeeze().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_retrain(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss_avg = train_loss / len(trainloader)\n",
    "    train_acc = train_correct / len(trainset)\n",
    "    train_losses.append(train_loss_avg)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # Evaluating on test set\n",
    "    model_retrain.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.squeeze().to(device)\n",
    "            outputs = model_retrain(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    test_acc = test_correct / test_total\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    # Adjusting learning rate\n",
    "    scheduler.step(test_acc)\n",
    "\n",
    "    print(f'\\nEpoch {epoch+1}/{fineepochs}:')\n",
    "    print(f'Train Loss: {train_loss_avg:.4f} | Train Acc: {train_acc:.4f}')\n",
    "    print(f'Test Acc : {test_acc:.4f}')\n",
    "etime_end=  time.time()\n",
    "time1 = etime_end - etime1\n",
    "epoc_t = time1/fineepochs\n",
    "print(epoc_t)\n",
    "metric_retrain = evaluate_model_with_metric(model_retrain, testloader , device)\n",
    "print(f\"Accuracy  : {metric_retrain['Accuracy (%)']:.2f}%\")\n",
    "print(f\"Precision (macro  : {metric_retrain['Precision (macro, %)']:.2f}%\")\n",
    "print(f\"F1-score (macro, %)  : {metric_retrain['F1-score (macro, %)']:.2f}%\")\n",
    "print(f\"Top-3 Accuracy (%)  : {metric_retrain['Top-3 Accuracy (%)']:.2f}%\")"
   ],
   "id": "3909a1f199f2e61f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import soundfile as sf\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# # Create directory to save samples\n",
    "# output_dir = \"interesting_samples\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "#\n",
    "# # Get paths to original test files (assuming organized by class)\n",
    "# test_files = []\n",
    "# for label, cmd in enumerate(COMMANDS):\n",
    "#     cmd_dir = os.path.join(DATA_DIR, cmd)\n",
    "#     for fn in os.listdir(cmd_dir):\n",
    "#         if fn.endswith('.wav'):\n",
    "#             test_files.append((os.path.join(cmd_dir, fn), label))\n",
    "#\n",
    "# # Note: Since we used train_test_split, need to match original files with test set samples\n",
    "# # This is simplified - adjust according to your actual data split method\n",
    "# # Assuming testset samples maintain original file order\n",
    "#\n",
    "# # Collect qualifying samples\n",
    "# interesting_samples = []\n",
    "#\n",
    "# model.eval()\n",
    "# model_copy.eval()\n",
    "# model_copy_fine_all.eval()\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     for idx, (inputs, true_label) in enumerate(testloader):\n",
    "#         inputs = inputs.to(device)\n",
    "#         true_label = true_label.view(-1).to(device)\n",
    "#\n",
    "#         # Original model prediction\n",
    "#         outputs = model(inputs)\n",
    "#         _, pred_original = torch.max(outputs, 1)\n",
    "#\n",
    "#         # Pruned model prediction\n",
    "#         outputs_pruned = model_copy(inputs)\n",
    "#         _, pred_pruned = torch.max(outputs_pruned, 1)\n",
    "#\n",
    "#         # Finetuned model prediction\n",
    "#         outputs_finetuned = model_copy_fine_all(inputs)\n",
    "#         _, pred_finetuned = torch.max(outputs_finetuned, 1)\n",
    "#\n",
    "#         # Check condition: original model wrong but others correct\n",
    "#         for i in range(inputs.size(0)):\n",
    "#             sample_idx = idx * testloader.batch_size + i\n",
    "#             if sample_idx >= len(testset):\n",
    "#                 break\n",
    "#\n",
    "#             if (pred_original[i] != true_label[i] and\n",
    "#                 pred_pruned[i] == true_label[i] and\n",
    "#                 pred_finetuned[i] == true_label[i]):\n",
    "#\n",
    "#                 # Get original audio path\n",
    "#                 audio_path, _ = test_files[sample_idx % len(test_files)]  # Simplified matching\n",
    "#\n",
    "#                 # Load original audio\n",
    "#                 audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE)\n",
    "#\n",
    "#                 # Save sample info\n",
    "#                 sample_info = {\n",
    "#                     'index': sample_idx,\n",
    "#                     'true_label': true_label[i].item(),\n",
    "#                     'true_class': COMMANDS[true_label[i].item()],\n",
    "#                     'original_pred': pred_original[i].item(),\n",
    "#                     'original_class': COMMANDS[pred_original[i].item()],\n",
    "#                     'audio_path': audio_path,\n",
    "#                     'audio_data': audio,\n",
    "#                     'sample_rate': sr,\n",
    "#                     'mfcc': inputs[i].cpu().numpy()\n",
    "#                 }\n",
    "#                 interesting_samples.append(sample_info)\n",
    "#\n",
    "# # Save interesting samples\n",
    "# print(f\"Found {len(interesting_samples)} interesting samples\")\n",
    "#\n",
    "# for i, sample in enumerate(interesting_samples):\n",
    "#     # Save audio file\n",
    "#     output_path = os.path.join(output_dir, f\"sample_{i}_{sample['true_class']}_origpred_{sample['original_class']}.wav\")\n",
    "#     sf.write(output_path, sample['audio_data'], sample['sample_rate'])\n",
    "#\n",
    "#     # Save MFCC visualization\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     librosa.display.specshow(sample['mfcc'], x_axis='time')\n",
    "#     plt.colorbar()\n",
    "#     plt.title(f\"MFCC - True: {sample['true_class']}, Orig Pred: {sample['original_class']}\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(os.path.join(output_dir, f\"sample_{i}_mfcc.png\"))\n",
    "#     plt.close()\n",
    "#\n",
    "#     # Save metadata\n",
    "#     with open(os.path.join(output_dir, f\"sample_{i}_info.txt\"), 'w') as f:\n",
    "#         f.write(f\"True label: {sample['true_class']} ({sample['true_label']})\\n\")\n",
    "#         f.write(f\"Original model prediction: {sample['original_class']} ({sample['original_pred']})\\n\")\n",
    "#         f.write(f\"Audio file: {sample['audio_path']}\\n\")\n",
    "#\n",
    "# print(f\"Saved {len(interesting_samples)} interesting samples to {output_dir}\")"
   ],
   "id": "bd4e4b36a9589a93",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
