{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "trainepochs = 60\n",
    "fineepochs = 30\n",
    "\n",
    "# Gaussian noise function\n",
    "def apply_gaussian_noise(image, noise_level=3.0):\n",
    "    \"\"\"Apply Gaussian noise to a single image\"\"\"\n",
    "    image = image.clone()  # Copy image to avoid modifying original data\n",
    "    noise = torch.randn(image.shape) * noise_level  # Generate Gaussian noise\n",
    "    noisy_image = torch.clamp(image + noise, 0, 1)  # Clip pixel values\n",
    "    return noisy_image\n",
    "\n",
    "# CIFAR-10 data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))  # CIFAR-10 normalization\n",
    "])\n",
    "\n",
    "# Load full CIFAR-10 dataset\n",
    "full_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get all labels\n",
    "labels = [y for _, y in full_trainset]\n",
    "\n",
    "# Stratified sampling\n",
    "_, subset_indices = train_test_split(\n",
    "    range(len(full_trainset)),\n",
    "    test_size=1/10,\n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")\n",
    "subset_indices = range(len(full_trainset))\n",
    "trainset = Subset(full_trainset, subset_indices)\n",
    "\n",
    "# Check category distribution\n",
    "from collections import Counter\n",
    "print(\"category distribution:\", Counter([full_trainset[i][1] for i in subset_indices]))\n",
    "\n",
    "# Generate indices for attacked samples\n",
    "num_train_attack = len(trainset) // 2  # 50% of training set to be attacked\n",
    "attack_train_indices = set(random.sample(range(len(trainset)), num_train_attack))  # Randomly select samples to add noise\n",
    "train_clean_indices = set(range(len(trainset))) - attack_train_indices  # Other half remains clean\n",
    "\n",
    "# Custom dataset class\n",
    "class NoisyCIFAR10Dataset(Dataset):\n",
    "    def __init__(self, dataset, attacked_indices, noise_level=0.1):\n",
    "        self.dataset = dataset\n",
    "        self.attacked_indices = attacked_indices\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        if idx in self.attacked_indices:\n",
    "            img = apply_gaussian_noise(img, self.noise_level)  # Add Gaussian noise\n",
    "        return img, label\n",
    "\n",
    "# Create attacked training set\n",
    "trainset_noisy = NoisyCIFAR10Dataset(trainset, attack_train_indices, noise_level=8)  # Use smaller noise level for CIFAR-10\n",
    "\n",
    "# Create DataLoaders\n",
    "trainloader = DataLoader(trainset_noisy, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Print counts of attacked vs clean samples\n",
    "print(f\"Train Clean Samples: {len(train_clean_indices)}, Attacked Samples: {len(attack_train_indices)}\")\n",
    "\n",
    "# Visualization function\n",
    "def show_images(dataset, indices, title):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "    for ax, idx in zip(axes, list(indices)[:5]):  # Show first 5 samples\n",
    "        img, label = dataset[idx]\n",
    "        # Adjust channel order for CIFAR-10 display\n",
    "        img = img.permute(1, 2, 0)  # Change from (C,H,W) to (H,W,C)\n",
    "        ax.imshow(img.numpy().clip(0, 1))  # Ensure values between 0-1\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Display sample attacked and clean images\n",
    "show_images(trainset_noisy, attack_train_indices, \"Attacked Train Samples\")\n",
    "show_images(trainset_noisy, train_clean_indices, \"Clean Train Samples\")\n",
    "\n",
    "# Split training set into attacked and clean subsets\n",
    "train_attack_subset = Subset(trainset_noisy, list(attack_train_indices))  # Attacked samples\n",
    "train_clean_subset = Subset(trainset_noisy, list(train_clean_indices))    # Clean samples\n",
    "\n",
    "# Create corresponding DataLoaders\n",
    "attackTrainloader = DataLoader(train_attack_subset, batch_size=64, shuffle=False)\n",
    "cleanTrainloader = DataLoader(train_clean_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Print subset sizes\n",
    "print(f\"Train - Attack: {len(train_attack_subset)}, Clean: {len(train_clean_subset)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import matplotlib.pyplot as plt\n",
    "# # import time\n",
    "# import torch.nn.functional as F\n",
    "#\n",
    "# # Ensure the plot background is white\n",
    "# plt.rcParams['figure.facecolor'] = 'white'\n",
    "#\n",
    "# # ---------- 1. Define Model Architecture ----------\n",
    "# class BasicBlock(nn.Module):\n",
    "#     expansion = 1\n",
    "#     def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
    "#         self.bn1   = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu  = nn.ReLU(inplace=True)\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "#         self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "#         self.downsample = downsample\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "#         out = self.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.bn2(self.conv2(out))\n",
    "#         if self.downsample:\n",
    "#             identity = self.downsample(x)\n",
    "#         out = self.relu(out + identity)\n",
    "#         return out\n",
    "#\n",
    "# class ResNet18(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.in_channels = 64\n",
    "#         self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "#         self.bn1   = nn.BatchNorm2d(64)\n",
    "#         self.layer1 = self._make_layer(64, 2)\n",
    "#         self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "#         self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "#         self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "#         self.fc1 = nn.Linear(512 * BasicBlock.expansion, 256)\n",
    "#         self.fc2 = nn.Linear(256, num_classes)\n",
    "#\n",
    "#     def _make_layer(self, out_channels, blocks, stride=1):\n",
    "#         downsample = None\n",
    "#         if stride != 1 or self.in_channels != out_channels * BasicBlock.expansion:\n",
    "#             downsample = nn.Sequential(\n",
    "#                 nn.Conv2d(self.in_channels, out_channels * BasicBlock.expansion, 1, stride, bias=False),\n",
    "#                 nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "#             )\n",
    "#         layers = [BasicBlock(self.in_channels, out_channels, stride, downsample)]\n",
    "#         self.in_channels = out_channels * BasicBlock.expansion\n",
    "#         for _ in range(1, blocks):\n",
    "#             layers.append(BasicBlock(self.in_channels, out_channels))\n",
    "#         return nn.Sequential(*layers)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.bn1(self.conv1(x)))\n",
    "#         x = self.layer1(x); x = self.layer2(x)\n",
    "#         x = self.layer3(x); x = self.layer4(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         return self.fc2(x)\n",
    "#\n",
    "# # ---------- 2. Instantiate Model, Optimizer, etc. ----------\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = ResNet18(num_classes=10).to(device)\n",
    "#\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)\n",
    "#\n",
    "# # ---------- 3. Training and Testing Parameters ----------\n",
    "# num_epochs = 150\n",
    "# train_losses = []\n",
    "# test_accuracies = []\n",
    "#\n",
    "# # ---------- 4. Training + Testing Loop ----------\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, targets in trainloader:\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#     avg_loss = running_loss / len(trainloader)\n",
    "#     train_losses.append(avg_loss)\n",
    "#\n",
    "#     # Test\n",
    "#     model.eval()\n",
    "#     correct, total = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in testloader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, pred = outputs.max(1)\n",
    "#             total += targets.size(0)\n",
    "#             correct += pred.eq(targets).sum().item()\n",
    "#     acc = 100.0 * correct / total\n",
    "#     test_accuracies.append(acc)\n",
    "#\n",
    "#     scheduler.step()\n",
    "#     print(f\"Epoch {epoch+1:3d}/{num_epochs} → Loss: {avg_loss:.4f}, Test Acc: {acc:.2f}%\")\n",
    "#\n",
    "# # ---------- 5. Plot and Save ----------\n",
    "# # Test Accuracy\n",
    "# plt.figure()\n",
    "# plt.plot(range(1, num_epochs+1), test_accuracies)\n",
    "# plt.title('Test Accuracy over Epochs')\n",
    "# plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)')\n",
    "# plt.grid(True)\n",
    "# plt.savefig('accuracy.png', dpi=300, facecolor='white')\n",
    "# plt.show()\n",
    "#\n",
    "# # Training Loss\n",
    "# plt.figure()\n",
    "# plt.plot(range(1, num_epochs+1), train_losses)\n",
    "# plt.title('Training Loss over Epochs')\n",
    "# plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.savefig('loss.png', dpi=300, facecolor='white')\n",
    "# plt.show()\n",
    "#\n",
    "# print(\"Generated and saved：accuracy.png, loss.png\")\n"
   ],
   "id": "35e33f3e9b58517a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Basic Residual Block\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# ResNet18 Model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Residual block layers\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # Classification layers\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, 256)\n",
    "        self.fc2 = nn.Linear(256,num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # Can pool\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def ResNet18_CIFAR10(num_classes=10):\n",
    "    return ResNet18(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNet18_CIFAR10().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)\n",
    "\n",
    "# Example training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# Test function\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    accuracy = 100.*correct/total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return  accuracy\n",
    "\n",
    "# Training process\n",
    "best_acc, total_train_time = 0, 0\n",
    "\n",
    "for epoch in range(trainepochs):\n",
    "    print(f\"Epoch {epoch+1}/{trainepochs}\")\n",
    "    start_time = time.time()\n",
    "    train(epoch)\n",
    "    scheduler.step()\n",
    "    best_acc = test()\n",
    "    epoch_time = time.time() - start_time\n",
    "    total_train_time += epoch_time\n",
    "\n",
    "# Final output results\n",
    "print(f\"\\nTraining completed with results:\")\n",
    "print(f\"Total training time: {total_train_time:.2f} seconds\")\n",
    "print(f\"Average time per epoch: {total_train_time/trainepochs:.2f}s\")\n",
    "print(f\"Best test accuracy: {best_acc:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f24f27f6f381b5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_ffn_attribution(model, data_loader):\n",
    "    \"\"\"\n",
    "    Compute attribution for the fc layer (nn.Linear(512, 256)) in ResNet18\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained ResNet18 model\n",
    "        data_loader: Data loader\n",
    "\n",
    "    Returns:\n",
    "        attributions: Attribution matrix of shape (num_samples, 256)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    attributions = []\n",
    "\n",
    "    for images, _ in data_loader:  # Labels not needed\n",
    "        images = images.to(device)\n",
    "        images.requires_grad_()  # Need to compute input gradients\n",
    "\n",
    "       # Forward pass up to before the fc layer\n",
    "        x = model.conv1(images)\n",
    "        x = F.relu(model.bn1(x))\n",
    "        x = model.layer1(x)\n",
    "        x = model.layer2(x)\n",
    "        x = model.layer3(x)\n",
    "        x = model.layer4(x)\n",
    "        x = model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)  # (batch_size, 512)\n",
    "\n",
    "        # Save input to the fc layer\n",
    "        fc_input = x.clone()\n",
    "\n",
    "        # Continue forward pass through fc layer\n",
    "        fc_output = model.fc(x)  # (batch_size, 256)\n",
    "\n",
    "        # Continue to final output\n",
    "        final_output = model.fc2(F.relu(fc_output))  # (batch_size, 100)\n",
    "\n",
    "        # Compute gradient matrix\n",
    "        gradient_matrix = []\n",
    "        for i in range(10):  # CIFAR100 has 100 classes\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Compute gradient of ith class output w.r.t fc layer output\n",
    "            grad_i = torch.autograd.grad(\n",
    "                outputs=final_output[:, i],\n",
    "                inputs=fc_output,\n",
    "                grad_outputs=torch.ones_like(final_output[:, i]),\n",
    "                retain_graph=True,\n",
    "                create_graph=False\n",
    "            )[0]  # (batch_size, 256)\n",
    "\n",
    "            gradient_matrix.append(grad_i)\n",
    "\n",
    "        # Stack gradients (batch_size, 100, 256)\n",
    "        gradient_matrix = torch.stack(gradient_matrix, dim=1)\n",
    "\n",
    "        # Use max aggregation (batch_size, 256)\n",
    "        attribution_max = (gradient_matrix.max(dim=1)[0] * fc_output).abs()\n",
    "        \n",
    "        attributions.append(attribution_max.detach().cpu())\n",
    "    \n",
    "    return torch.cat(attributions, dim=0)  # (num_samples, 256)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c152b741ba2d6941",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Automatically select device\n",
    "model.to(device)  # Move model to GPU\n",
    "# Compute attribution\n",
    "attributions = compute_ffn_attribution(model, trainloader)\n",
    "\n",
    "# attributions shape is (num_samples, 256), representing each sample's importance on 256-dimensional features\n",
    "attributions.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "971ee5d5a85963ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume ffn_attributions is a PyTorch tensor on GPU\n",
    "# 1. Move tensor from GPU to CPU\n",
    "ffn_attributions_cpu = attributions.cpu()\n",
    "\n",
    "# 2. Convert PyTorch tensor to NumPy array\n",
    "ffn_attributions_np = ffn_attributions_cpu.numpy()\n",
    "\n",
    "# 3. Standardize using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(ffn_attributions_np)\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# GMM clustering\n",
    "gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(X_scaled)\n",
    "\n",
    "# Get indices for two clusters\n",
    "cluster_0_indices = np.where(gmm_labels == 0)[0]\n",
    "cluster_1_indices = np.where(gmm_labels == 1)[0]\n",
    "\n",
    "print(f\"\\nGMM classification results:\")\n",
    "print(f\"Cluster 0 sample count: {len(cluster_0_indices)}\")\n",
    "print(f\"Cluster 1 sample count: {len(cluster_1_indices)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a34d378caaf79d4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate the matching between K-Means clustering and manual partitioning\n",
    "attack_set = set(attack_train_indices)  # True attack sample indices\n",
    "clean_set = set(train_clean_indices)    # True clean sample indices\n",
    "\n",
    "cluster_0_set = set(cluster_0_indices)  # Samples in K-Means cluster 0\n",
    "cluster_1_set = set(cluster_1_indices)  # Samples in K-Means cluster 1\n",
    "\n",
    "# Calculate intersections\n",
    "attack_in_cluster_0 = len(attack_set & cluster_0_set)  # Number of true attack samples in Cluster 0\n",
    "attack_in_cluster_1 = len(attack_set & cluster_1_set)  # Number of true attack samples in Cluster 1\n",
    "\n",
    "clean_in_cluster_0 = len(clean_set & cluster_0_set)  # Number of true clean samples in Cluster 0\n",
    "clean_in_cluster_1 = len(clean_set & cluster_1_set)  # Number of true clean samples in Cluster 1\n",
    "\n",
    "# Calculate classification accuracy of K-Means for attack samples\n",
    "attack_accuracy_cluster_0 = (attack_in_cluster_0 / len(attack_set)) * 100\n",
    "attack_accuracy_cluster_1 = (attack_in_cluster_1 / len(attack_set)) * 100\n",
    "\n",
    "clean_accuracy_cluster_0 = (clean_in_cluster_0 / len(clean_set)) * 100\n",
    "clean_accuracy_cluster_1 = (clean_in_cluster_1 / len(clean_set)) * 100\n",
    "\n",
    "# Print comparison results\n",
    "print(\"==== K-Means Clustering vs Manual Labels ====\")\n",
    "print(f\"Cluster 0: {len(cluster_0_indices)} samples\")\n",
    "print(f\"Cluster 1: {len(cluster_1_indices)} samples\\n\")\n",
    "\n",
    "print(f\"Attack Samples in Cluster 0: {attack_in_cluster_0} ({attack_accuracy_cluster_0:.2f}%)\")\n",
    "print(f\"Attack Samples in Cluster 1: {attack_in_cluster_1} ({attack_accuracy_cluster_1:.2f}%)\\n\")\n",
    "\n",
    "print(f\"Clean Samples in Cluster 0: {clean_in_cluster_0} ({clean_accuracy_cluster_0:.2f}%)\")\n",
    "print(f\"Clean Samples in Cluster 1: {clean_in_cluster_1} ({clean_accuracy_cluster_1:.2f}%)\")\n",
    "\n",
    "# Calculate overall classification accuracy of K-Means\n",
    "total_correct = attack_in_cluster_0 + clean_in_cluster_1  # Assuming cluster_0 mainly contains attack samples, cluster_1 mainly clean samples\n",
    "overall_accuracy = (total_correct / len(trainset)) * 100  # Previously train_subset, now changed to trainset\n",
    "\n",
    "print(f\"\\nOverall Clustering Accuracy: {overall_accuracy:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2caa388dd83d784",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract samples from cluster 0\n",
    "cluster_0_attributions = attributions[cluster_0_indices]\n",
    "\n",
    "# Extract samples from cluster 1\n",
    "cluster_1_attributions = attributions[cluster_1_indices]\n",
    "print(cluster_0_attributions.shape)\n",
    "print(cluster_1_attributions.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cd8e9adcb91b0db",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Ensure labels length matches dataset size gam_labels\n",
    "#assert len(labels) == len(trainloader.dataset), \"labels and trainloader.dataset size mismatch!\"\n",
    "\n",
    "# Convert indices to Python list\n",
    "cluster_0_indices = cluster_0_indices.tolist()\n",
    "cluster_1_indices = cluster_1_indices.tolist()\n",
    "\n",
    "# Use Subset to split original dataset by indices\n",
    "dataset_0 = Subset(trainloader.dataset, cluster_0_indices)\n",
    "dataset_1 = Subset(trainloader.dataset, cluster_1_indices)\n",
    "\n",
    "# Create new DataLoaders\n",
    "trainloader_0 = DataLoader(dataset_0, batch_size=64, shuffle=True)\n",
    "trainloader_1 = DataLoader(dataset_1, batch_size=64, shuffle=True)\n",
    "\n",
    "# Final verification\n",
    "print(f\"Expected Cluster 0 size: {len(cluster_0_indices)}, Actual: {len(trainloader_0.dataset)}\")\n",
    "print(f\"Expected Cluster 1 size: {len(cluster_1_indices)}, Actual: {len(trainloader_1.dataset)}\")\n",
    "\n",
    "# Assign datasets based on clustering accuracy\n",
    "if overall_accuracy > 0.5:\n",
    "    fine_tuning_load = trainloader_1\n",
    "    noisy_dataset = trainloader_0\n",
    "else:\n",
    "    fine_tuning_load = trainloader_0\n",
    "    noisy_dataset = trainloader_1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abeb7a0fb5dac62e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Calculate model accuracy on specified dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained PyTorch model\n",
    "    - dataloader: DataLoader for evaluation\n",
    "    - device: Computation device ('cuda' or 'cpu')\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Accuracy score (float)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for faster inference\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8c094f32059f719",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate_model_with_metrics(model, dataloader, device, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculate multiple evaluation metrics on specified dataset: Accuracy, Precision, Recall, F1.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained PyTorch model\n",
    "        dataloader: DataLoader for evaluation\n",
    "        device: Computation device ('cuda' or 'cpu')\n",
    "        average: Averaging method for precision/recall/f1, options: 'macro', 'micro', 'weighted'\n",
    "\n",
    "    Returns:\n",
    "        metrics: dict containing:\n",
    "            - accuracy\n",
    "            - precision\n",
    "            - recall\n",
    "            - f1\n",
    "            - confusion_matrix (as numpy array)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate into 1D arrays\n",
    "    y_true = torch.cat(all_labels).numpy()\n",
    "    y_pred = torch.cat(all_preds).numpy()\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = (y_pred == y_true).mean()\n",
    "    precision = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ],
   "id": "f044ba4640a879c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate model on cluster_0 and cluster_1 datasets\n",
    "accuracy_cluster_0 = evaluate_model(model, trainloader_0, device)\n",
    "accuracy_cluster_1 = evaluate_model(model, trainloader_1, device)\n",
    "accuracy_cluster = evaluate_model(model,trainloader,device)\n",
    "\n",
    "print(f\"Accuracy on Cluster 0: {accuracy_cluster_0:.2f}%\")\n",
    "print(f\"Accuracy on Cluster 1: {accuracy_cluster_1:.2f}%\")\n",
    "print(f\"Accuracy on Cluster : {accuracy_cluster:.2f}%\")\n",
    "\n",
    "accuracy_attack_train = evaluate_model(model, attackTrainloader , device)\n",
    "accuracy_clean_train = evaluate_model(model, cleanTrainloader , device)\n",
    "# accuracy_attack_test = evaluate_model(model, attackTestloader , device)\n",
    "accuracy_test = evaluate_model(model, testloader , device)\n",
    "\n",
    "print(f\"Accuracy on attack_train : {accuracy_attack_train:.2f}%\")\n",
    "print(f\"Accuracy on clean_train : {accuracy_clean_train:.2f}%\")\n",
    "\n",
    "print(f\"Accuracy on test : {accuracy_test:.2f}%\")\n",
    "\n",
    "#print(f\"Accuracy on clean_test : {accuracy_clean_test:.2f}%\")\n",
    "accuracy_source = accuracy_test\n",
    "metric_test = evaluate_model_with_metrics(model,testloader,device,average='macro')\n",
    "print(f\"Accuracy:  {metric_test['accuracy']*100:.2f}%\")\n",
    "print(f\"Precision: {metric_test['precision']:.4f}\")\n",
    "print(f\"Recall:    {metric_test['recall']:.4f}\")\n",
    "print(f\"F1-score:  {metric_test['f1']:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9781fc920196b965",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "# Create a deep copy of the model\n",
    "model_copy = copy.deepcopy(model)\n",
    "\n",
    "# Ensure the new model is on the same device as the original\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy.to(device)\n",
    "\n",
    "# Verify successful copying\n",
    "print(model_copy)\n",
    "# Operate on model-copy version2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c3bb4dcb40fb2f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_layer_features(model, data_loader, cluster_labels=None, layer_name='fc_512_256', overall_accuracy=1.0):\n",
    "    \"\"\"\n",
    "    Extract features from specified fully-connected layer in ResNet18\n",
    "\n",
    "    Parameters:\n",
    "        model: ResNet18 model instance\n",
    "        data_loader: DataLoader returning (images, indices) tuples\n",
    "        cluster_labels: Optional array of cluster labels (0 or 1)\n",
    "        layer_name: Currently only supports 'fc_512_256' (corresponds to nn.Linear(512, 256))\n",
    "        overall_accuracy: Current model accuracy used to determine label inversion\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix (n_samples, 256)\n",
    "        y: Label array if cluster_labels provided, otherwise None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    if cluster_labels is not None and isinstance(cluster_labels, torch.Tensor):\n",
    "        cluster_labels = cluster_labels.cpu().numpy()\n",
    "\n",
    "    for images, indices in data_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass up to fc layer\n",
    "            x = model.conv1(images)\n",
    "            x = F.relu(model.bn1(x))\n",
    "            x = model.layer1(x)\n",
    "            x = model.layer2(x)\n",
    "            x = model.layer3(x)\n",
    "            x = model.layer4(x)\n",
    "            x = model.avgpool(x)\n",
    "            x = torch.flatten(x, 1)  # (batch_size, 512)\n",
    "\n",
    "            # Extract fc layer features\n",
    "            if layer_name == 'fc_512_256':\n",
    "                features = model.fc(x)  # (batch_size, 256)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer name: {layer_name}. Only 'fc_512_256' is supported.\")\n",
    "\n",
    "        # Store features\n",
    "        X_list.append(features.cpu().numpy())\n",
    "\n",
    "        # Process labels if cluster_labels provided\n",
    "        if cluster_labels is not None:\n",
    "            batch_labels = cluster_labels[indices.numpy()]\n",
    "            if overall_accuracy > 0.5:\n",
    "                inverse_labels = 1 - batch_labels\n",
    "            else:\n",
    "                inverse_labels = batch_labels\n",
    "            y_list.append(inverse_labels)\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0) if cluster_labels is not None else None\n",
    "\n",
    "    return X, y"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "580564a4b431949a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "X_1,y_1 = compute_layer_features(model_copy, trainloader,gmm_labels)\n",
    "#X_2,y_2 = compute_layer_features(model_copy, trainloader,gmm_labels,\"fc_512_256\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae28b3b418929f0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# def solve_linear_regression_torch(X, y):\n",
    "#     X = torch.tensor(X, dtype=torch.float32)\n",
    "#     y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "#\n",
    "#     # Add bias term\n",
    "#     ones = torch.ones(X.shape[0], 1)\n",
    "#     X_bias = torch.cat([X, ones], dim=1)  # Shape becomes (60000, 65)\n",
    "#\n",
    "#     # Solve using least squares\n",
    "#     W = torch.linalg.lstsq(X_bias, y).solution\n",
    "#     return W[:-1], W[-1]  # W[:-1] are weights (n_features,), W[-1] is bias\n",
    "# #W2_torch, b2_torch = solve_linear_regression_torch(X_2, y_2)\n",
    "# W1_torch, b1_torch = solve_linear_regression_torch(X_1, y_1)\n",
    "#\n",
    "# print(\"W1 shape:\", W1_torch.shape)  # (64,)\n",
    "# print(\"b:\", b1_torch)\n",
    "# # To print all weights:\n",
    "# # for i, w_i in enumerate(W2_torch):\n",
    "# #     print(f\"W[{i}] = {w_i}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c531c61cb00a6608",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # Modify fc1 according to fc2 with 40 neurons\n",
    "# # Flatten W_torch into a one-dimensional tensor\n",
    "# W_torch = W1_torch.flatten()  # shape is (128,)\n",
    "#\n",
    "# # Sort in descending order by absolute value\n",
    "# W_abs_sorted, indices = torch.sort(torch.abs(W_torch), descending=True)\n",
    "#\n",
    "# # Take the indices of the top 10 largest neurons\n",
    "# top_10_indices = indices[:40]  # Take the first 10 indices 40 / 128\n",
    "# top_10_values = W_torch[top_10_indices]  ## Get the corresponding W values\n",
    "#\n",
    "# # Print the contents of top_10_values\n",
    "# print(\"Top 10 values:\", top_10_values)\n",
    "#\n",
    "# # output\n",
    "# print(\"Top 10 neurons with highest absolute weights:\")\n",
    "# for rank, (idx, val) in enumerate(zip(top_10_indices.tolist(), top_10_values.tolist()), start=1):\n",
    "#     print(f\"Rank {rank}: Neuron {idx}, Weight = {float(val):.6f}\")  # Ensure val is a float How to prune\n",
    "# # Get the weights and biases of the fc2 layer\n",
    "# fc1_weight = model_copy.fc.weight  # Weight matrix, shape (out_features, in_features)\n",
    "# fc1_bias = model_copy.fc.bias      # Bias vector, shape (out_features,)\n",
    "#\n",
    "# # Assume top_10_indices are the indices of the top 10 neurons obtained earlier\n",
    "# #top_10_indices = torch.tensor([12, 45, 3, 28, 7, 19, 33, 56, 22, 41])  # Example data\n",
    "# neurons_to_zero = top_10_indices[:40]  # Take the first 7 neurons\n",
    "# print(\"Neurons to zero:\", neurons_to_zero)\n",
    "#\n",
    "# # Gradually set the weights and biases to 0\n",
    "# for neuron_idx in neurons_to_zero:\n",
    "#     # Set the corresponding neuron's weights to 0\n",
    "#     fc1_weight.data[neuron_idx, :] = 0  # Set the weight row of this neuron to 0\n",
    "#     # Set the corresponding neuron's bias to 0\n",
    "#     #fc2_bias.data[neuron_idx] = 0\n",
    "#     print(f\"Neuron {neuron_idx} weight and bias set to 0.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c22347820cc7aaff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model_copy.fc.weight.shape"
   ],
   "id": "842afed301660c7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def magnitude_based_pruning(model, layer_name='fc', pruning_rate=0.2, pruning_type='global'):\n",
    "#     \"\"\"\n",
    "#     Weight magnitude-based pruning method (applied to specified layer)\n",
    "#\n",
    "#     Args:\n",
    "#         model: Model to be pruned\n",
    "#         layer_name: Target layer name (consistent 'fc' layer as original method)\n",
    "#         pruning_rate: Pruning ratio (0-1)\n",
    "#         pruning_type: 'global' (global pruning) or 'local' (local pruning)\n",
    "#     \"\"\"\n",
    "#     # Get target layer\n",
    "#     layer = getattr(model, layer_name)\n",
    "#     weight = layer.weight.data\n",
    "#     print(layer)\n",
    "#     # print(weight)\n",
    "#     if pruning_type == 'global':\n",
    "#          # ----------------------------\n",
    "#         # Global pruning strategy (entire weight matrix)\n",
    "#         # ----------------------------\n",
    "#         # Flatten weights and calculate number of weights to prune\n",
    "#         flat_weights = weight.view(-1)\n",
    "#         num_prune = int(pruning_rate * flat_weights.numel())\n",
    "#\n",
    "#         # Find indices of k% smallest absolute weights\n",
    "#         _, indices = torch.topk(flat_weights.abs(), num_prune, largest=False)\n",
    "#\n",
    "#         # Zero out selected weights\n",
    "#         flat_weights[indices] = 0\n",
    "#         layer.weight.data = flat_weights.view_as(weight)\n",
    "#\n",
    "#     elif pruning_type == 'local':\n",
    "#         # ----------------------------\n",
    "#         # Local pruning strategy (per-neuron pruning)\n",
    "#         # ----------------------------\n",
    "#         for neuron_idx in range(weight.size(0)):\n",
    "#             # Get current neuron's input weights\n",
    "#             neuron_weights = weight[neuron_idx]\n",
    "#\n",
    "#             # Calculate number of weights to prune\n",
    "#             num_prune_neuron = int(pruning_rate * neuron_weights.numel())\n",
    "#\n",
    "#             # Find smallest absolute weights for current neuron\n",
    "#             _, indices = torch.topk(neuron_weights.abs(), num_prune_neuron, largest=False)\n",
    "#\n",
    "#             # Zero out selected weights\n",
    "#             neuron_weights[indices] = 0\n",
    "#\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported pruning type: {pruning_type}\")\n",
    "#\n",
    "# # Usage example (applied to same layer as original method)\n",
    "# # --------------------------------------------------\n",
    "# # Global pruning: prune 40% of smallest absolute weights in fc layer\n",
    "# magnitude_based_pruning(model_copy, layer_name='fc', pruning_rate=0.5, pruning_type='global')\n",
    "#\n",
    "# # Local pruning: prune 30% of input connections per neuron\n",
    "# # magnitude_based_pruning(model_copy, layer_name='fc', pruning_rate=0.3, pruning_type='local')"
   ],
   "id": "9e1314dbf2e7d301",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def hessian_based_pruning(model, dataloader, layer_name='fc', pruning_rate=0.2, device='cuda'):\n",
    "    \"\"\"Hessian matrix-based approximate importance pruning\"\"\"\n",
    "    model.eval()\n",
    "    layer = getattr(model, layer_name)\n",
    "    weight = layer.weight.data\n",
    "\n",
    "    # Compute second-order derivative approximation\n",
    "    hessian_diag = torch.zeros_like(weight)\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, targets, reduction='sum')\n",
    "        grad = torch.autograd.grad(loss, layer.weight, create_graph=True)\n",
    "        hessian_diag += torch.autograd.grad(grad, layer.weight, grad_outputs=torch.ones_like(grad))[0].abs()\n",
    "\n",
    "    # Select least important weights\n",
    "    flat_hessian = hessian_diag.view(-1)\n",
    "    num_prune = int(pruning_rate * flat_hessian.numel())\n",
    "    _, indices = torch.topk(flat_hessian, num_prune, largest=False)\n",
    "\n",
    "    # Apply pruning\n",
    "    flat_weights = layer.weight.data.view(-1)\n",
    "    flat_weights[indices] = 0\n",
    "    layer.weight.data = flat_weights.view_as(weight)"
   ],
   "id": "9dcbfe22e09d2254",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hessian_based_pruning(model_copy,fine_tuning_load)"
   ],
   "id": "a006ded26ab7b394",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Prune testing\n",
    "# Ensure the data is on the GPU (if available)\n",
    "# Test on the pruned model_copy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy.to(device)\n",
    "\n",
    "# Evaluate the model on the cluster_0 and cluster_1 datasets\n",
    "accuracy_cluster_0 = evaluate_model(model_copy, trainloader_0, device)\n",
    "accuracy_cluster_1 = evaluate_model(model_copy, trainloader_1, device)\n",
    "accuracy_cluster = evaluate_model(model_copy,trainloader,device)\n",
    "\n",
    "print(f\"Accuracy on Cluster 0: {accuracy_cluster_0:.2f}%\")\n",
    "print(f\"Accuracy on Cluster 1: {accuracy_cluster_1:.2f}%\")\n",
    "print(f\"Accuracy on Cluster : {accuracy_cluster:.2f}%\")\n",
    "\n",
    "accuracy_attack_train = evaluate_model(model_copy, attackTrainloader , device)\n",
    "accuracy_clean_train = evaluate_model(model_copy, cleanTrainloader , device)\n",
    "# accuracy_attack_test = evaluate_model(model, attackTestloader , device)\n",
    "accuracy_test = evaluate_model(model_copy, testloader , device)\n",
    "\n",
    "print(f\"Accuracy on attack_train : {accuracy_attack_train:.2f}%\")\n",
    "print(f\"Accuracy on clean_train : {accuracy_clean_train:.2f}%\")\n",
    "accuracy_prune = accuracy_test\n",
    "\n",
    "print(f\"Accuracy on test : {accuracy_test:.2f}%\")\n",
    "metric_prune = evaluate_model_with_metrics(model_copy,testloader,device,average='macro')\n",
    "print(f\"Accuracy:  {metric_prune['accuracy']*100:.2f}%\")\n",
    "print(f\"Precision: {metric_prune['precision']:.4f}\")\n",
    "print(f\"Recall:    {metric_prune['recall']:.4f}\")\n",
    "print(f\"F1-score:  {metric_prune['f1']:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6185fb3cff97b0e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "accuracy_prune"
   ],
   "id": "416a55f022485986",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "# Deep copy the model\n",
    "model_copy_fine_all = copy.deepcopy(model_copy)\n",
    "\n",
    "# Ensure the new model is on the same device as the original\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy_fine_all.to(device)\n",
    "\n",
    "# Verify if the copy was successful\n",
    "print(model_copy_fine_all)\n",
    "# Preparation for fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b01cf510d6c136a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Assume device is GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model_copy = model_copy.to(device)\n",
    "\n",
    "# Freeze all layers except fc2\n",
    "for name, param in model_copy.named_parameters():\n",
    "    if not (name.startswith(\"fc\") or name.startswith(\"fc2\")):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Check which layers' parameters are frozen\n",
    "for name, param in model_copy.named_parameters():\n",
    "    print(f\"{name}: requires_grad = {param.requires_grad}\")\n",
    "\n",
    "# Define optimizer - only optimize parameters that require gradients\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model_copy.parameters()),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-5  # Add weight decay to prevent overfitting\n",
    ")\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "import time\n",
    "\n",
    "# Training loop (with time statistics)\n",
    "total_training_time = 0  # Track total training time\n",
    "\n",
    "for epoch in range(fineepochs):\n",
    "    epoch_start_time = time.time()  # Record epoch start time\n",
    "    model_copy.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    batch_times = []  # Track time per batch\n",
    "\n",
    "    for inputs, labels in fine_tuning_load:\n",
    "        batch_start_time = time.time()  # Record batch start time\n",
    "\n",
    "        # Move input data and labels to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_copy(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Record batch time\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        batch_times.append(batch_time)\n",
    "\n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    total_training_time += epoch_time\n",
    "\n",
    "    # Calculate average batch time\n",
    "    avg_batch_time = sum(batch_times)/len(batch_times) if batch_times else 0\n",
    "\n",
    "    # Print training info (with time statistics)\n",
    "    print(f\"Epoch [{epoch + 1}/{fineepochs}], \"\n",
    "          f\"Loss: {running_loss / len(trainloader):.4f}, \"\n",
    "          f\"Epoch Time: {epoch_time:.2f}s, \"\n",
    "          f\"Avg Batch Time: {avg_batch_time*1000:.1f}ms\")\n",
    "\n",
    "# Final time statistics\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
    "print(f\"Average epoch time: {total_training_time/fineepochs:.2f}s\")\n",
    "time_fc1_fc2 = total_training_time/fineepochs\n",
    "\n",
    "# Validate model\n",
    "model_copy.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for inputs, labels in testloader:\n",
    "        # Move input data and labels to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_copy(inputs)\n",
    "\n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Count correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2aaa4feb98cbbaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Assume device is GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model_copy_fine_all = model_copy_fine_all.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model_copy_fine_all.parameters(), lr=0.001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "import time\n",
    "\n",
    "# Training loop (with time tracking)\n",
    "total_training_time = 0  # Track total training time\n",
    "\n",
    "for epoch in range(fineepochs):\n",
    "    epoch_start_time = time.time()  # Record epoch start time\n",
    "    model_copy_fine_all.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    batch_times = []  # Track time per batch\n",
    "    \n",
    "    for inputs, labels in fine_tuning_load:\n",
    "        batch_start_time = time.time()  # Record batch start time\n",
    "\n",
    "        # Move inputs and labels to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_copy_fine_all(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Track batch time\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        batch_times.append(batch_time)\n",
    "\n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    total_training_time += epoch_time\n",
    "\n",
    "    # Calculate average batch time\n",
    "    avg_batch_time = sum(batch_times)/len(batch_times) if batch_times else 0\n",
    "\n",
    "    # Print training info (with timing)\n",
    "    print(f\"Epoch [{epoch + 1}/{fineepochs}], \"\n",
    "          f\"Loss: {running_loss / len(trainloader):.4f}, \"\n",
    "          f\"Epoch Time: {epoch_time:.2f}s, \"\n",
    "          f\"Avg Batch Time: {avg_batch_time*1000:.1f}ms\")\n",
    "\n",
    "# Final timing statistics\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
    "print(f\"Average epoch time: {total_training_time/fineepochs:.2f}s\")\n",
    "time_all = total_training_time/fineepochs\n",
    "\n",
    "# Validate model\n",
    "model_copy_fine_all.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for inputs, labels in testloader:\n",
    "        # Move inputs and labels to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_copy_fine_all(inputs)\n",
    "\n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Count correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd54ebd02757f360",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Ensure data is on GPU (if available) - Testing on fine-tuned model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_copy.to(device)\n",
    "\n",
    "# Evaluate model on cluster_0 and cluster_1 datasets\n",
    "accuracy_cluster_0 = evaluate_model(model_copy, trainloader_0, device)\n",
    "accuracy_cluster_1 = evaluate_model(model_copy, trainloader_1, device)\n",
    "accuracy_cluster = evaluate_model(model_copy,trainloader,device)\n",
    "\n",
    "print(f\"Accuracy on Cluster 0: {accuracy_cluster_0:.2f}%\")\n",
    "print(f\"Accuracy on Cluster 1: {accuracy_cluster_1:.2f}%\")\n",
    "print(f\"Accuracy on Cluster : {accuracy_cluster:.2f}%\")\n",
    "\n",
    "accuracy_attack_train = evaluate_model(model_copy, attackTrainloader , device)\n",
    "accuracy_clean_train = evaluate_model(model_copy, cleanTrainloader , device)\n",
    "# accuracy_attack_test = evaluate_model(model, attackTestloader , device)\n",
    "accuracy_test = evaluate_model(model_copy, testloader , device)\n",
    "accuracy_test_all = evaluate_model(model_copy_fine_all, testloader , device)\n",
    "print(f\"Accuracy on attack_train : {accuracy_attack_train:.2f}%\")\n",
    "print(f\"Accuracy on clean_train : {accuracy_clean_train:.2f}%\")\n",
    "#print(f\"Accuracy on test souse : {accuracy_test:.2f}%\")\n",
    "\n",
    "print(f\"Accuracy on test : {accuracy_source:.2f}%\")\n",
    "print(f\"Accuracy on test_prune : {accuracy_prune:.2f}%\")\n",
    "print(f\"Average epoch time on special: {time_fc1_fc2:.2f}s\")\n",
    "print(f\"Accuracy on test_fin-tuning special : {accuracy_test:.2f}%\")\n",
    "print(f\"Average epoch time on all: {time_all:.2f}s\")\n",
    "print(f\"Accuracy on test_fin-tuning all : {accuracy_test_all:.2f}%\")\n",
    "metric_fin_single = evaluate_model_with_metrics(model_copy, testloader , device)\n",
    "metric_fin_all = evaluate_model_with_metrics(model_copy_fine_all, testloader , device)\n",
    "print(\"single fine -tuning \")\n",
    "print(f\"Accuracy:  {metric_fin_single['accuracy']*100:.2f}%\")\n",
    "print(f\"Precision: {metric_fin_single['precision']:.4f}\")\n",
    "print(f\"Recall:    {metric_fin_single['recall']:.4f}\")\n",
    "print(f\"F1-score:  {metric_fin_single['f1']:.4f}\")\n",
    "\n",
    "print(\"All fine -tuning \")\n",
    "print(f\"Accuracy:  {metric_fin_all['accuracy']*100:.2f}%\")\n",
    "print(f\"Precision: {metric_fin_all['precision']:.4f}\")\n",
    "print(f\"Recall:    {metric_fin_all['recall']:.4f}\")\n",
    "print(f\"F1-score:  {metric_fin_all['f1']:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8166f5645b29e6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "def ResNet18_CIFAR10(num_classes=10):\n",
    "    return ResNet18(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model_fresh = ResNet18_CIFAR10(num_classes=10).to(device)\n",
    "optimizer_fresh = optim.SGD(model_fresh.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler_fresh = MultiStepLR(optimizer_fresh, milestones=[60, 120, 160], gamma=0.2)\n",
    "criterion_fresh = nn.CrossEntropyLoss()\n",
    "\n",
    "# Record training loss, test accuracy and time\n",
    "train_losses_fresh = []\n",
    "test_accuracies_fresh = []\n",
    "epoch_times = []  # Record total time per epoch\n",
    "train_times = []  # Record training time per epoch\n",
    "test_times = []   # Record testing time per epoch\n",
    "\n",
    "for epoch in range(fineepochs):\n",
    "    start_time = time.time()  # Record epoch start time\n",
    "\n",
    "    # Training\n",
    "    model_fresh.train()\n",
    "    running_loss = 0.0\n",
    "    train_start = time.time()\n",
    "    for inputs, targets in trainloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer_fresh.zero_grad()\n",
    "        outputs = model_fresh(inputs)\n",
    "        loss = criterion_fresh(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_fresh.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(trainloader)\n",
    "    train_losses_fresh.append(avg_loss)\n",
    "    train_end = time.time()\n",
    "    train_time = train_end - train_start\n",
    "    train_times.append(train_time)\n",
    "\n",
    "    # Testing\n",
    "    model_fresh.eval()\n",
    "    correct, total = 0, 0\n",
    "    test_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model_fresh(inputs)\n",
    "            _, pred = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += pred.eq(targets).sum().item()\n",
    "    acc = 100.0 * correct / total\n",
    "    test_accuracies_fresh.append(acc)\n",
    "    test_end = time.time()\n",
    "    test_time = test_end - test_start\n",
    "    test_times.append(test_time)\n",
    "\n",
    "    # Calculate total time\n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler_fresh.step()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"[Fresh] Epoch {epoch+1:3d}/{fineepochs} → \"\n",
    "          f\"Loss: {avg_loss:.4f}, Test Acc: {acc:.2f}%, \"\n",
    "          f\"Train Time: {train_time:.2f}s, Test Time: {test_time:.2f}s, \"\n",
    "          f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "\n",
    "# Calculate average times\n",
    "avg_train_time = sum(train_times) / len(train_times)\n",
    "avg_test_time = sum(test_times) / len(test_times)\n",
    "avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "\n",
    "print(f\"\\nAverage Time per Epoch → \"\n",
    "      f\"Train: {avg_train_time:.2f}s, Test: {avg_test_time:.2f}s, \"\n",
    "      f\"Total: {avg_epoch_time:.2f}s\")\n",
    "\n",
    "# Evaluate final model\n",
    "accuracy_model_fresh = evaluate_model(model_fresh, testloader, device)\n",
    "print(f\"Accuracy on fresh model: {accuracy_model_fresh:.2f}%\")\n",
    "metric_fresh = evaluate_model_with_metrics(model_fresh, testloader, device)\n",
    "print(\"Fresh model \")\n",
    "print(f\"Accuracy:  {metric_fresh['accuracy']*100:.2f}%\")\n",
    "print(f\"Precision: {metric_fresh['precision']:.4f}\")\n",
    "print(f\"Recall:    {metric_fresh['recall']:.4f}\")\n",
    "print(f\"F1-score:  {metric_fresh['f1']:.4f}\")"
   ],
   "id": "40c0731f74e78262",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
